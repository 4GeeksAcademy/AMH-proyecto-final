{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting missingno\n",
      "  Downloading missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xgboost\n",
      "  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib->missingno) (24.1)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/vscode/.local/lib/python3.10/site-packages (from matplotlib->missingno) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting nvidia-nccl-cu12\n",
      "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, tqdm, threadpoolctl, pyparsing, pillow, nvidia-nccl-cu12, numpy, kiwisolver, joblib, fonttools, cycler, click, scipy, pandas, contourpy, xgboost, scikit-learn, matplotlib, lightgbm, seaborn, lazypredict, missingno\n",
      "Successfully installed click-8.1.7 contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 joblib-1.4.2 kiwisolver-1.4.7 lazypredict-0.2.12 lightgbm-4.5.0 matplotlib-3.9.2 missingno-0.5.2 numpy-2.1.2 nvidia-nccl-cu12-2.23.4 pandas-2.2.3 pillow-10.4.0 pyparsing-3.1.4 pytz-2024.2 scikit-learn-1.5.2 scipy-1.14.1 seaborn-0.13.2 threadpoolctl-3.5.0 tqdm-4.66.5 tzdata-2024.2 xgboost-2.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=3.10.0\n",
      "  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow) (67.8.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1\n",
      "  Downloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /home/vscode/.local/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.18,>=2.17\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/vscode/.local/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Collecting numpy<2.0.0,>=1.23.5\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras>=3.2.0\n",
      "  Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/vscode/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Collecting rich\n",
      "  Downloading rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting optree\n",
      "  Downloading optree-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vscode/.local/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, astunparse, absl-py, werkzeug, requests, ml-dtypes, markdown-it-py, h5py, tensorboard, rich, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed MarkupSafe-3.0.1 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.8.30 charset-normalizer-3.4.0 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.66.2 h5py-3.12.1 idna-3.10 keras-3.6.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 opt-einsum-3.4.0 optree-0.13.0 protobuf-4.25.5 requests-2.32.3 rich-13.9.2 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 urllib3-2.2.3 werkzeug-3.0.4 wrapt-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install missingno lazypredict\n",
    "!pip install tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspaces/AMH-proyecto-final_albaebauluz/data/DF_modelos/df_residencial_total_ALBA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>año</th>\n",
       "      <th>fecha</th>\n",
       "      <th>findesemana</th>\n",
       "      <th>festivos</th>\n",
       "      <th>lectivos</th>\n",
       "      <th>covid</th>\n",
       "      <th>tmed</th>\n",
       "      <th>prec</th>\n",
       "      <th>velmedia</th>\n",
       "      <th>poblacion</th>\n",
       "      <th>pib</th>\n",
       "      <th>rentanetamediaporpersona</th>\n",
       "      <th>rentanetamediaporhogar</th>\n",
       "      <th>bono_social</th>\n",
       "      <th>sector_economic</th>\n",
       "      <th>consumo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.590909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.417647</td>\n",
       "      <td>5628011</td>\n",
       "      <td>62490</td>\n",
       "      <td>14656.0</td>\n",
       "      <td>38386.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Residencial</td>\n",
       "      <td>6713701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.617391</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.152941</td>\n",
       "      <td>5628011</td>\n",
       "      <td>62490</td>\n",
       "      <td>14656.0</td>\n",
       "      <td>38386.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Residencial</td>\n",
       "      <td>7354663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.604348</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.676471</td>\n",
       "      <td>5628011</td>\n",
       "      <td>62490</td>\n",
       "      <td>14656.0</td>\n",
       "      <td>38386.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Residencial</td>\n",
       "      <td>7619413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.004348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.335294</td>\n",
       "      <td>5628011</td>\n",
       "      <td>63436</td>\n",
       "      <td>14656.0</td>\n",
       "      <td>38386.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Residencial</td>\n",
       "      <td>7815514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.173913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.247059</td>\n",
       "      <td>5628011</td>\n",
       "      <td>62490</td>\n",
       "      <td>14656.0</td>\n",
       "      <td>38386.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Residencial</td>\n",
       "      <td>7527041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      año       fecha  findesemana  festivos  lectivos  covid      tmed  prec  \\\n",
       "0  2019.0  2019-01-01            0         1         0      0  8.590909   0.0   \n",
       "1  2019.0  2019-01-02            0         0         0      0  6.617391   0.3   \n",
       "2  2019.0  2019-01-03            0         0         0      0  5.604348   0.1   \n",
       "3  2019.0  2019-01-04            0         0         0      0  5.004348   0.0   \n",
       "4  2019.0  2019-01-05            1         0         0      0  6.173913   0.0   \n",
       "\n",
       "   velmedia  poblacion    pib  rentanetamediaporpersona  \\\n",
       "0  1.417647    5628011  62490                   14656.0   \n",
       "1  2.152941    5628011  62490                   14656.0   \n",
       "2  1.676471    5628011  62490                   14656.0   \n",
       "3  1.335294    5628011  63436                   14656.0   \n",
       "4  2.247059    5628011  62490                   14656.0   \n",
       "\n",
       "   rentanetamediaporhogar  bono_social sector_economic  consumo  \n",
       "0                 38386.0            0     Residencial  6713701  \n",
       "1                 38386.0            0     Residencial  7354663  \n",
       "2                 38386.0            0     Residencial  7619413  \n",
       "3                 38386.0            0     Residencial  7815514  \n",
       "4                 38386.0            0     Residencial  7527041  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Preparación de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "año                         float64\n",
       "fecha                        object\n",
       "findesemana                   int64\n",
       "festivos                      int64\n",
       "lectivos                      int64\n",
       "covid                         int64\n",
       "tmed                        float64\n",
       "prec                        float64\n",
       "velmedia                    float64\n",
       "poblacion                     int64\n",
       "pib                           int64\n",
       "rentanetamediaporpersona    float64\n",
       "rentanetamediaporhogar      float64\n",
       "bono_social                   int64\n",
       "sector_economic              object\n",
       "consumo                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "año                         1\n",
      "fecha                       0\n",
      "findesemana                 0\n",
      "festivos                    0\n",
      "lectivos                    0\n",
      "covid                       0\n",
      "tmed                        0\n",
      "prec                        0\n",
      "velmedia                    0\n",
      "poblacion                   0\n",
      "pib                         0\n",
      "rentanetamediaporpersona    0\n",
      "rentanetamediaporhogar      0\n",
      "bono_social                 0\n",
      "sector_economic             0\n",
      "consumo                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1970, 16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Rentanetamediaporhogar': 'renta_hogar'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dividir las variables independientes y la variable dependiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tmed', 'pib']]  # Variables independientes (continuas y dummy)\n",
    "y = df['consumo']  # Variable dependiente (demanda de electricidad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dividir el conjunto de datos en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Escalar las variables continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Definir la arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo secuencial\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada (input) y capas ocultas\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Capa de salida (una neurona para predicción continua)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 36654471446528.0000 - val_loss: 38133311733760.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35898930495488.0000 - val_loss: 38133206876160.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 36353341390848.0000 - val_loss: 38132804222976.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36625652383744.0000 - val_loss: 38131671760896.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 36643377512448.0000 - val_loss: 38129117429760.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36616726904832.0000 - val_loss: 38124205899776.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 36545667006464.0000 - val_loss: 38115758571520.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 36807102169088.0000 - val_loss: 38102382936064.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36645931843584.0000 - val_loss: 38082552266752.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36998509232128.0000 - val_loss: 38054492372992.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37204801880064.0000 - val_loss: 38016244514816.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 36284462530560.0000 - val_loss: 37965925449728.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36702152294400.0000 - val_loss: 37901379305472.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 36197174870016.0000 - val_loss: 37819795898368.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 36349314859008.0000 - val_loss: 37718985801728.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 36202375806976.0000 - val_loss: 37597292265472.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36192603078656.0000 - val_loss: 37450755866624.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35845411176448.0000 - val_loss: 37278986534912.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35398784909312.0000 - val_loss: 37078284894208.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35265947107328.0000 - val_loss: 36846411186176.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35737584009216.0000 - val_loss: 36580064493568.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35145943875584.0000 - val_loss: 36279429365760.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 34722098970624.0000 - val_loss: 35941565595648.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 34166345302016.0000 - val_loss: 35563310678016.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34223266201600.0000 - val_loss: 35146222796800.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33321954312192.0000 - val_loss: 34684157296640.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33013498904576.0000 - val_loss: 34176338231296.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 33076474281984.0000 - val_loss: 33625106022400.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32014338097152.0000 - val_loss: 33032580890624.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 31251574554624.0000 - val_loss: 32392406368256.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30668077662208.0000 - val_loss: 31703991058432.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 29837492224000.0000 - val_loss: 30971107737600.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29322282795008.0000 - val_loss: 30190608580608.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28821937979392.0000 - val_loss: 29368680185856.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28055695261696.0000 - val_loss: 28504162828288.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26757524619264.0000 - val_loss: 27606684532736.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25792270565376.0000 - val_loss: 26665646292992.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25210919059456.0000 - val_loss: 25691022163968.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23974471794688.0000 - val_loss: 24687652372480.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 22876268789760.0000 - val_loss: 23652072423424.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21742137376768.0000 - val_loss: 22592893222912.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20805901615104.0000 - val_loss: 21519407251456.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20232357806080.0000 - val_loss: 20428242288640.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18797522059264.0000 - val_loss: 19333057085440.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17839201189888.0000 - val_loss: 18232274583552.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16790990094336.0000 - val_loss: 17137754177536.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15590225870848.0000 - val_loss: 16047999549440.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 14683727724544.0000 - val_loss: 14969330466816.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 13795782033408.0000 - val_loss: 13908512342016.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12615124254720.0000 - val_loss: 12880076341248.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 11870147706880.0000 - val_loss: 11879107788800.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10712881561600.0000 - val_loss: 10918406651904.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9762423963648.0000 - val_loss: 9998526578688.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8997541249024.0000 - val_loss: 9109431648256.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8177646043136.0000 - val_loss: 8277830139904.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7237717196800.0000 - val_loss: 7494540197888.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6729252732928.0000 - val_loss: 6761744957440.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5697592360960.0000 - val_loss: 6092591464448.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5094911246336.0000 - val_loss: 5465203277824.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4708246749184.0000 - val_loss: 4894514741248.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4271215738880.0000 - val_loss: 4386570108928.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3655527038976.0000 - val_loss: 3924816822272.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3316812873728.0000 - val_loss: 3514197868544.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2936072568832.0000 - val_loss: 3147941019648.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2756407197696.0000 - val_loss: 2833213816832.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2294608297984.0000 - val_loss: 2559740739584.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2240212107264.0000 - val_loss: 2320308633600.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1919452708864.0000 - val_loss: 2115256713216.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1775341404160.0000 - val_loss: 1938492620800.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1596647014400.0000 - val_loss: 1786503364608.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1487343845376.0000 - val_loss: 1662325096448.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1329847205888.0000 - val_loss: 1556786315264.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1247088082944.0000 - val_loss: 1466894516224.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1159948009472.0000 - val_loss: 1394138808320.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1242789576704.0000 - val_loss: 1332678885376.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1176357437440.0000 - val_loss: 1277658398720.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1101992951808.0000 - val_loss: 1234657476608.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 973755383808.0000 - val_loss: 1196574769152.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 870444957696.0000 - val_loss: 1165700759552.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1053414260736.0000 - val_loss: 1140301234176.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134948515840.0000 - val_loss: 1116966879232.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1078137716736.0000 - val_loss: 1096645607424.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 869846941696.0000 - val_loss: 1077259206656.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1016174215168.0000 - val_loss: 1062918488064.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1024231931904.0000 - val_loss: 1049120210944.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 992767115264.0000 - val_loss: 1036298682368.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 900291297280.0000 - val_loss: 1025128267776.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 987370029056.0000 - val_loss: 1015152377856.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 949210251264.0000 - val_loss: 1006434516992.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 891156037632.0000 - val_loss: 997592727552.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 910591066112.0000 - val_loss: 989866491904.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 793464733696.0000 - val_loss: 982429597696.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 841249849344.0000 - val_loss: 974607482880.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1067286265856.0000 - val_loss: 968549400576.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 892316352512.0000 - val_loss: 962417459200.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 842466197504.0000 - val_loss: 956009545728.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 809921347584.0000 - val_loss: 949622407168.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 809018916864.0000 - val_loss: 944209068032.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1001641869312.0000 - val_loss: 938970775552.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 993602306048.0000 - val_loss: 933418565632.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 807861682176.0000\n",
      "Test Loss: 877378994176.0\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/13\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MAE: 584026.7893401015\n",
      "RMSE: 936685.1009202775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizar el progreso del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHJCAYAAAB5WBhaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABxJklEQVR4nO3dd3wUdf7H8ddueg8lFQKEDqE3BZQiKCDSREV+qGAHAQuWO8+GlfPsDVFP4SyooIAUFRAElCbSuzSpSaipkLY7vz8m2SQkQBKSbLJ5Px+PuZ3MfGfms3NI3sx8Z74WwzAMRERERFyE1dkFiIiIiJQmhRsRERFxKQo3IiIi4lIUbkRERMSlKNyIiIiIS1G4EREREZeicCMiIiIuReFGREREXIrCjYiIiLgUhRuRCm7UqFHUq1fP2WWUSI8ePejRo0e5H7ewc2axWJg4ceIlt504cSIWi6VU61m2bBkWi4Vly5aV6n5FpHAKNyIlZLFYijTpF9qFbdiwAYvFwtNPP33BNnv27MFisTBhwoRyrKxkJk+ezLRp05xdRj49evSgRYsWzi5DpFy5O7sAkcrqiy++yPfz559/zuLFiwssb9as2WUd55NPPsFut1/WPiqqdu3a0bRpU77++mteeumlQttMnz4dgNtuu+2yjnXu3Dnc3cv2r7zJkydTs2ZNRo0alW95t27dOHfuHJ6enmV6fBExKdyIlND5v2zXrFnD4sWLL/lL+OzZs/j6+hb5OB4eHiWqr7IYMWIEzzzzDGvWrOHKK68ssP7rr7+madOmtGvX7rKO4+3tfVnbXw6r1erU44tUNbotJVKGcm4JrF+/nm7duuHr68u//vUvAH744Qf69+9PZGQkXl5eNGjQgBdffBGbzZZvH+f3H/n777+xWCy8/vrrfPzxxzRo0AAvLy86duzIunXrLlnT6dOneeyxx2jZsiX+/v4EBgbSr18/Nm/enK9dTj+RGTNm8PLLL1O7dm28vb3p1asXe/fuLbDfnFp8fHzo1KkTv/32W5HO0YgRI4DcKzR5rV+/nt27dzvaFPWcFaawPje///47HTt2xNvbmwYNGvDRRx8Vuu3UqVO55pprCA0NxcvLi+bNm/Phhx/ma1OvXj22b9/O8uXLHbckc/obXajPzcyZM2nfvj0+Pj7UrFmT2267jaNHj+ZrM2rUKPz9/Tl69CiDBw/G39+fkJAQHnvssSJ976KaPHkyMTExeHl5ERkZydixY0lISMjXZs+ePQwdOpTw8HC8vb2pXbs2t956K4mJiY42ixcv5qqrriI4OBh/f3+aNGni+DMvUl505UakjJ06dYp+/fpx6623cttttxEWFgbAtGnT8Pf3Z8KECfj7+7N06VKeffZZkpKSeO211y653+nTp5OcnMz999+PxWLhP//5DzfeeCP79++/6NWe/fv3M2fOHG6++Waio6OJj4/no48+onv37uzYsYPIyMh87f/9739jtVp57LHHSExM5D//+Q8jRoxg7dq1jjaffvop999/P126dOHhhx9m//79DBw4kOrVqxMVFXXR7xEdHU2XLl2YMWMGb731Fm5ubvm+I8D//d//lco5y2vr1q1cd911hISEMHHiRLKysnjuuecc///k9eGHHxITE8PAgQNxd3dn3rx5PPDAA9jtdsaOHQvA22+/zfjx4/H39+epp54CKHRfOaZNm8add95Jx44dmTRpEvHx8bzzzjusXLmSjRs3Ehwc7Ghrs9no06cPV1xxBa+//jq//PILb7zxBg0aNGDMmDHF+t6FmThxIs8//zy9e/dmzJgx7N69mw8//JB169axcuVKPDw8yMjIoE+fPqSnpzN+/HjCw8M5evQo8+fPJyEhgaCgILZv384NN9xAq1ateOGFF/Dy8mLv3r2sXLnysmsUKRZDRErF2LFjjfP/k+revbsBGFOmTCnQ/uzZswWW3X///Yavr6+RlpbmWDZy5Eijbt26jp8PHDhgAEaNGjWM06dPO5b/8MMPBmDMmzfvonWmpaUZNpst37IDBw4YXl5exgsvvOBY9uuvvxqA0axZMyM9Pd2x/J133jEAY+vWrYZhGEZGRoYRGhpqtGnTJl+7jz/+2ACM7t27X7QewzCMDz74wACMhQsXOpbZbDajVq1aRufOnR3LSnrODMMwAOO5555z/Dx48GDD29vbOHjwoGPZjh07DDc3twL/PxZ23D59+hj169fPtywmJqbQ75tzLn/99VfDMHLPWYsWLYxz58452s2fP98AjGeffTbfdwHy/X9jGIbRtm1bo3379gWOdb7u3bsbMTExF1x//Phxw9PT07juuuvy/bl4//33DcD47LPPDMMwjI0bNxqAMXPmzAvu66233jIA48SJE5esS6Qs6baUSBnz8vLizjvvLLDcx8fHMZ+cnMzJkye5+uqrOXv2LLt27brkfocNG0a1atUcP1999dWAeWXmUvVYreZ/+jabjVOnTjluH2zYsKFA+zvvvDNfR9jzj/Pnn39y/PhxRo8ena/dqFGjCAoKuuT3yPkuHh4e+W5NLV++nKNHjzpuScHln7McNpuNhQsXMnjwYOrUqeNY3qxZM/r06VOgfd7jJiYmcvLkSbp3787+/fvz3ZIpqpxz9sADD+Tri9O/f3+aNm3KggULCmwzevTofD9fffXVl/z/uih++eUXMjIyePjhhx1/LgDuvfdeAgMDHbXk/H+5cOFCzp49W+i+cq42/fDDDy7bCV4qhyodblasWMGAAQOIjIzEYrEwZ86cYm2flpbGqFGjaNmyJe7u7gwePLhAm1mzZnHttdcSEhJCYGAgnTt3ZuHChaXzBaRSqFWrVqFPyWzfvp0hQ4YQFBREYGAgISEhjs7IRfmFmfeXMuAIOmfOnLnodna7nbfeeotGjRrh5eVFzZo1CQkJYcuWLYUe91LHOXjwIACNGjXK187Dw4P69etf8nsA1KhRgz59+jB79mzS0tIA85aUu7s7t9xyi6Pd5Z6zHCdOnODcuXMFagZo0qRJgWUrV66kd+/e+Pn5ERwcTEhIiKMfSUnCTc45K+xYTZs2dazP4e3tTUhISL5l1apVu+T/15dTi6enJ/Xr13esj46OZsKECfz3v/+lZs2a9OnThw8++CDf9x82bBhdu3blnnvuISwsjFtvvZUZM2Yo6Ei5q9LhJjU1ldatW/PBBx+UaHubzYaPjw8PPvggvXv3LrTNihUruPbaa/nxxx9Zv349PXv2ZMCAAWzcuPFySpdKJO+/+nMkJCTQvXt3Nm/ezAsvvMC8efNYvHgxr776KkCRfhnk7ZuSl2EYF93ulVdeYcKECXTr1o0vv/yShQsXsnjxYmJiYgo9bkmPU1y33XYbSUlJzJ8/n4yMDL7//ntHnxgonXNWEvv27aNXr16cPHmSN998kwULFrB48WIeeeSRMj1uXhf6/6C8vfHGG2zZsoV//etfnDt3jgcffJCYmBiOHDkCmH/WV6xYwS+//MLtt9/Oli1bGDZsGNdee22pdn4WuZQq3aG4X79+9OvX74Lr09PTeeqpp/j6669JSEigRYsWvPrqq44nIPz8/BxPTKxcubLAkwVgdjLM65VXXuGHH35g3rx5tG3btrS+ilQyy5Yt49SpU8yaNYtu3bo5lh84cKDMj/3dd9/Rs2dPPv3003zLExISqFmzZrH3V7duXcB8kuaaa65xLM/MzOTAgQO0bt26SPsZOHAgAQEBTJ8+HQ8PD86cOZPvllRpnrOQkBB8fHzYs2dPgXW7d+/O9/O8efNIT09n7ty5+a5i/frrrwW2LeqbjXPO2e7du/Ods5xlOevLQ95a8l5py8jI4MCBAwX+4dayZUtatmzJ008/zapVq+jatStTpkxxvKfIarXSq1cvevXqxZtvvskrr7zCU089xa+//nrBfwSKlLYqfeXmUsaNG8fq1av55ptv2LJlCzfffDN9+/Yt9C/EorLb7SQnJ1O9evVSrFQqm5x/iee9+pGRkcHkyZPL5djnX3WZOXNmgUeQi6pDhw6EhIQwZcoUMjIyHMunTZtWaOC/EB8fH4YMGcKPP/7Ihx9+iJ+fH4MGDcpXN5TOOXNzc6NPnz7MmTOHQ4cOOZbv3LmzwG3jwo6bmJjI1KlTC+zXz8+vSN+5Q4cOhIaGMmXKFNLT0x3Lf/rpJ3bu3En//v2L+5VKrHfv3nh6evLuu+/m+46ffvopiYmJjlqSkpLIysrKt23Lli2xWq2O73D69OkC+2/Tpg1Avu8pUtaq9JWbizl06BBTp07l0KFDjkdjH3vsMX7++WemTp3KK6+8UqL9vv7666SkpOTrRyBVT5cuXahWrRojR47kwQcfxGKx8MUXX5T6rZ7C3HDDDbzwwgvceeeddOnSha1bt/LVV18VuX/M+Tw8PHjppZe4//77ueaaaxg2bBgHDhxg6tSpxd7nbbfdxueff87ChQsZMWIEfn5+jnWlfc6ef/55fv75Z66++moeeOABsrKyeO+994iJiWHLli2Odtdddx2enp4MGDCA+++/n5SUFD755BNCQ0OJjY3Nt8/27dvz4Ycf8tJLL9GwYUNCQ0MLXJkB85y9+uqr3HnnnXTv3p3hw4c7HgWvV6+e45ZXaTlx4kShb4COjo5mxIgRPPnkkzz//PP07duXgQMHsnv3biZPnkzHjh0dfZqWLl3KuHHjuPnmm2ncuDFZWVl88cUXuLm5MXToUABeeOEFVqxYQf/+/albty7Hjx9n8uTJ1K5dm6uuuqpUv5PIRTntOa0KBjBmz57t+DnnkUw/P798k7u7u3HLLbcU2H7kyJHGoEGDLnqMr776yvD19TUWL15cytVLRXChR8Ev9BjuypUrjSuvvNLw8fExIiMjjSeeeMJYuHBhvkeGDePCj4K/9tprBfbJeY87FyYtLc149NFHjYiICMPHx8fo2rWrsXr1aqN79+75HmPOeXz5/Ed/c44/derUfMsnT55sREdHG15eXkaHDh2MFStWFNjnpWRlZRkREREGYPz4448F1pf0nBlG4edm+fLlRvv27Q1PT0+jfv36xpQpU4znnnuuwP+Pc+fONVq1amV4e3sb9erVM1599VXjs88+MwDjwIEDjnZxcXFG//79jYCAgHyPwZ//KHiOb7/91mjbtq3h5eVlVK9e3RgxYoRx5MiRfG1Gjhxp+Pn5FTgXhdVZmJzXERQ29erVy9Hu/fffN5o2bWp4eHgYYWFhxpgxY4wzZ8441u/fv9+46667jAYNGhje3t5G9erVjZ49exq//PKLo82SJUuMQYMGGZGRkYanp6cRGRlpDB8+3Pjrr78uWadIabIYRjn8U7ESsFgszJ492/HE07fffsuIESPYvn17gc58/v7+hIeH51s2atQoEhISLvjE1TfffMNdd93FzJkzy/WSs4iISFWj21IX0LZtW2w2G8ePH3e816Okvv76a+666y6++eYbBRsREZEyVqXDTUpKSr4xcg4cOMCmTZuoXr06jRs3ZsSIEdxxxx288cYbtG3blhMnTrBkyRJatWrlCCk7duwgIyOD06dPk5yczKZNm4DcTnTTp09n5MiRvPPOO1xxxRXExcUBZufJor7gTERERIquSt+WWrZsGT179iywfOTIkUybNo3MzExeeuklPv/8c44ePUrNmjW58soref7552nZsiVgDpZ3/gu3IPfJih49erB8+fILHkNERERKV5UONyIiIuJ69J4bERERcSkKNyIiIuJSqlyHYrvdzrFjxwgICCjyq9JFRETEuQzDIDk5mcjIyHwj2BemyoWbY8eOERUV5ewyREREpAQOHz5M7dq1L9qmyoWbgIAAwDw5gYGBTq5GREREiiIpKYmoqCjH7/GLqXLhJudWVGBgoMKNiIhIJVOULiXqUCwiIiIuReFGREREXIrCjYiIiLiUKtfnRkRELp/NZiMzM9PZZYiL8fT0vORj3kWhcCMiIkVmGAZxcXEkJCQ4uxRxQVarlejoaDw9PS9rPwo3IiJSZDnBJjQ0FF9fX70MVUpNzkt2Y2NjqVOnzmX92VK4ERGRIrHZbI5gU6NGDWeXIy4oJCSEY8eOkZWVhYeHR4n3ow7FIiJSJDl9bHx9fZ1cibiqnNtRNpvtsvajcCMiIsWiW1FSVkrrz5bCjYiIiLgUhRsREZFiqlevHm+//bazy5ALULgRERGXZbFYLjpNnDixRPtdt24d991332XV1qNHDx5++OHL2ocUTk9LlZasdEiJv8BKCzjuI2bPW6z5J6sbWN2zJw8ohZcYiYhUdbGxsY75b7/9lmeffZbdu3c7lvn7+zvmDcPAZrPh7n7pX40hISGlW6iUKoWb0hK7BT7tXXr7s7iBuze4e+V+egWAV2D2ZwD4BINfCPjVBL9Qcz6oNgREKByJiADh4eGO+aCgICwWi2PZsmXL6NmzJz/++CNPP/00W7duZdGiRURFRTFhwgTWrFlDamoqzZo1Y9KkSfTunft3fL169Xj44YcdV14sFguffPIJCxYsYOHChdSqVYs33niDgQMHlrj277//nmeffZa9e/cSERHB+PHjefTRRx3rJ0+ezFtvvcXhw4cJCgri6quv5rvvvgPgu+++4/nnn2fv3r34+vrStm1bfvjhB/z8/EpcT2WicFNaLBYzhBTGMHJmsucNMOzmdCGGDTJTzam43DwhKAqq1YXq9SEsBsJbQWhz8NQjnCJSOgzD4Fzm5T2yW1I+Hm6l9mTNP//5T15//XXq169PtWrVOHz4MNdffz0vv/wyXl5efP755wwYMIDdu3dTp06dC+7n+eef5z//+Q+vvfYa7733HiNGjODgwYNUr1692DWtX7+eW265hYkTJzJs2DBWrVrFAw88QI0aNRg1ahR//vknDz74IF988QVdunTh9OnT/Pbbb4B5tWr48OH85z//YciQISQnJ/Pbb79hOH4XuT6Fm9JSuwM8faHbUhdhGGC3mWHGnpU92czbXFlpeT7TID0F0pMgPdmczp2G1JOQesKcUuIh6RjYMuD0PnPatzTPwSxQo6FZa72roN7VZgASESmBc5k2mj+70CnH3vFCH3w9S+dX2AsvvMC1117r+Ll69eq0bt3a8fOLL77I7NmzmTt3LuPGjbvgfkaNGsXw4cMBeOWVV3j33Xf5448/6Nu3b7FrevPNN+nVqxfPPPMMAI0bN2bHjh289tprjBo1ikOHDuHn58cNN9xAQEAAdevWpW3btoAZbrKysrjxxhupW9f8O75ly5bFrqEyU7hxNosF3Nwx/6/wuvz92bIg+Ric+RvOHIRTeyBuG8RtMQPQqT3mtPlrs31QHYi+Gpr2hwa9wOMCV59ERFxUhw4d8v2ckpLCxIkTWbBggSMonDt3jkOHDl10P61atXLM+/n5ERgYyPHjx0tU086dOxk0aFC+ZV27duXtt9/GZrNx7bXXUrduXerXr0/fvn3p27cvQ4YMwdfXl9atW9OrVy9atmxJnz59uO6667jpppuoVq1aiWqpjBRuXI2bOwTXMafo89Ylx5sh5+Aq+Pt3OLYBEg/Bpq/MydMfGveF5oOg0bXg4eOUryAilYOPhxs7XujjtGOXlvP7oTz22GMsXryY119/nYYNG+Lj48NNN91ERkbGRfdz/nABFosFu/0i3Q8uQ0BAABs2bGDZsmUsWrSIZ599lokTJ7Ju3TqCg4NZvHgxq1atYtGiRbz33ns89dRTrF27lujo838xuCaFm6okIAwCrjWDC5i3uQ6vhT2LYedcSDoK274zJ+8gaHs7dLwHqleN/xhEpHgsFkup3RqqSFauXMmoUaMYMmQIYF7J+fvvv8u1hmbNmrFy5coCdTVu3Bg3NzPYubu707t3b3r37s1zzz1HcHAwS5cu5cYbb8RisdC1a1e6du3Ks88+S926dZk9ezYTJkwo1+/hLK73p1KKzssfGvYypz6vmFdydsyB7T+YV3RWvw+rP4DGfaDTveZtK712XURcXKNGjZg1axYDBgzAYrHwzDPPlNkVmBMnTrBp06Z8yyIiInj00Ufp2LEjL774IsOGDWP16tW8//77TJ48GYD58+ezf/9+unXrRrVq1fjxxx+x2+00adKEtWvXsmTJEq677jpCQ0NZu3YtJ06coFmzZmXyHSoihRsxWa1mR+PaHaD3C7B3MfzxMez9Bf762ZxqdYDeE80+OiIiLurNN9/krrvuokuXLtSsWZN//OMfJCUllcmxpk+fzvTp0/Mte/HFF3n66aeZMWMGzz77LC+++CIRERG88MILjBo1CoDg4GBmzZrFxIkTSUtLo1GjRnz99dfExMSwc+dOVqxYwdtvv01SUhJ169bljTfeoF+/fmXyHSoii1GVng0DkpKSCAoKIjExkcDAQGeXU/Gd3AvrPoENX+Q+lt6wN/R6DiJaXXxbEXEpaWlpHDhwgOjoaLy99fCBlL6L/Rkrzu9vvelNLq5mQ+j3Kjy0CTrea75Bee8v8NHVMOs+SDnh7ApFRETyUbiRovEPhf6vw9g/oMVQc9mWb+GDTrD5mzwvKhQREXEuhRspnhoN4KbP4N6lENbCfJHg7Pvhq5sg4eLvgBARESkPCjdSMrXaw33L4JpnwM3LvFX1wZVm3xwREREnUriRknPzgG6PwZiVUKez2eF47jiY+yBkpjm7OhERqaIUbuTy1WwEo36Ea54GLLDhfzC1LyQcdnZlIiJSBSncSOmwWqHb43Dbd+BTDY5thI+6nTdwp4iISNlTuJHS1bA33LccItqYnY2/HKp+OCIiUq4UbqT0VasLdy2E1sPBsJv9cFa95+yqRESkilC4kbLh4Q2DP4Qu482fFz0Nvzyv9+GISKXUo0cPHn74YcfP9erV4+23377oNhaLhTlz5lz2sUtrP1WJwo2UHYsFrn3RHKoB4Pc3Yf4jYLc5ty4RqTIGDBhA3759C13322+/YbFY2LJlS7H3u27dOu67777LLS+fiRMn0qZNmwLLY2Njy3xcqGnTphEcHFymxyhPCjdStiwWuHoC3PAWYIH1U2HOGCijEXZFRPK6++67Wbx4MUeOHCmwburUqXTo0IFWrYo/Tl5ISAi+vr6lUeIlhYeH4+XlVS7HchUKN1I+OtxlvtnY4mYO27DwX7pFJSJl7oYbbiAkJIRp06blW56SksLMmTO5++67OXXqFMOHD6dWrVr4+vrSsmVLvv7664vu9/zbUnv27KFbt254e3vTvHlzFi9eXGCbf/zjHzRu3BhfX1/q16/PM888Q2ZmJmBeOXn++efZvHkzFosFi8XiqPn821Jbt27lmmuuwcfHhxo1anDfffeRkpLiWD9q1CgGDx7M66+/TkREBDVq1GDs2LGOY5XEoUOHGDRoEP7+/gQGBnLLLbcQHx/vWL9582Z69uxJQEAAgYGBtG/fnj///BOAgwcPMmDAAKpVq4afnx8xMTH8+OOPJa6lKNzLdO8iebW4EWyZMPs+WPsh+NU0XwIoIpWTYUDmWecc28PXvDJ8Ce7u7txxxx1MmzaNp556Ckv2NjNnzsRmszF8+HBSUlJo3749//jHPwgMDGTBggXcfvvtNGjQgE6dOl3yGHa7nRtvvJGwsDDWrl1LYmJivv45OQICApg2bRqRkZFs3bqVe++9l4CAAJ544gmGDRvGtm3b+Pnnn/nll18ACAoKKrCP1NRU+vTpQ+fOnVm3bh3Hjx/nnnvuYdy4cfkC3K+//kpERAS//vore/fuZdiwYbRp04Z77733kt+nsO+XE2yWL19OVlYWY8eOZdiwYSxbtgyAESNG0LZtWz788EPc3NzYtGkTHh4eAIwdO5aMjAxWrFiBn58fO3bswN/fv9h1FIfCTSk5k5rByn0nL9rGgvkflcUCFnL+u7RgtYDVYsHNasFqNX92t1rxdLfg4WZ1TN4eVnw93fH1dMPL3er4j7RSaT3MfET853/C0hfBtwZ0uNPZVYlISWSehVcinXPsfx0DT78iNb3rrrt47bXXWL58OT169ADMW1JDhw4lKCiIoKAgHnss9x9a48ePZ+HChcyYMaNI4eaXX35h165dLFy4kMhI83y88sorBfrJPP300475evXq8dhjj/HNN9/wxBNP4OPjg7+/P+7u7oSHh1/wWNOnTyctLY3PP/8cPz/z+7///vsMGDCAV199lbCwMACqVavG+++/j5ubG02bNqV///4sWbKkROFmyZIlbN26lQMHDhAVFQXA559/TkxMDOvWraNjx44cOnSIxx9/nKZNmwLQqFEjx/aHDh1i6NChtGzZEoD69esXu4biUrgpJQdOpTJu+sZyO57FAn6e7gT5eBDk40Gwr/kZEuBFWKA3EUHehAd6ExHsQ1Q1H9zdKtAdyCvHQOpJ+O11WDABfKtD80HOrkpEXFTTpk3p0qULn332GT169GDv3r389ttvvPDCCwDYbDZeeeUVZsyYwdGjR8nIyCA9Pb3IfWp27txJVFSUI9gAdO7cuUC7b7/9lnfffZd9+/aRkpJCVlYWgYGBxfouO3fupHXr1o5gA9C1a1fsdju7d+92hJuYmBjc3NwcbSIiIti6dWuxjpX3mFFRUY5gA9C8eXOCg4PZuXMnHTt2ZMKECdxzzz188cUX9O7dm5tvvpkGDRoA8OCDDzJmzBgWLVpE7969GTp0aIn6ORWHwk0pCfBy58r61QssP79biZH9PwYGhmH+bDcM7AbY7QY2u4HdMMiyG2TZ7GTaDDJsdjKy7JzLtJGRZXfsNyU9i5T0LI4mnLtobR5uFupU96V+iD/1Q/xoHhFIm6hg6lT3dd7Vn2uehrOnzA7G398D/mFQ50rn1CIiJePha15Bcdaxi+Huu+9m/PjxfPDBB0ydOpUGDRrQvXt3AF577TXeeecd3n77bVq2bImfnx8PP/wwGRkZpVbu6tWrGTFiBM8//zx9+vQhKCiIb775hjfeeKPUjpFXzi2hHBaLBXsZPsgxceJE/u///o8FCxbw008/8dxzz/HNN98wZMgQ7rnnHvr06cOCBQtYtGgRkyZN4o033mD8+PFlVo9Tw82kSZOYNWsWu3btwsfHhy5duvDqq6/SpEmTC24zbdo07rwz/20MLy8v0tKcO1Bjo7AAvrmvYFIvbTa7wblMG2czskhNt5F4LpOEsxnZn5kcT04jLjGduKRzxCWmcTThHGmZdvadSGXfidR8+6rm60HrqGDaRlXjqkY1aRMVjJu1nMKOxQL934CzJ2HnPJhxh/lm48CI8jm+iFw+i6XIt4ac7ZZbbuGhhx5i+vTpfP7554wZM8bxj7uVK1cyaNAgbrvtNsDsY/LXX3/RvHnzIu27WbNmHD58mNjYWCIizL/D1qxZk6/NqlWrqFu3Lk899ZRj2cGDB/O18fT0xGa7+KsymjVrxrRp00hNTXVcvVm5ciVWq/WivzsvR873O3z4sOPqzY4dO0hISMh3jho3bkzjxo155JFHGD58OFOnTmXIkCEAREVFMXr0aEaPHs2TTz7JJ5984rrhZvny5YwdO5aOHTuSlZXFv/71L6677jp27NiR75Lb+QIDA9m9e7fj50rZ96SE3KwW/L3c8fdyh4BLt7fbDWKT0th/IoV9x1PYdyKVrUcT2XEsiTNnM1m2+wTLdp/grV/+ItjXg6sbhdC9cQg9m4RQw7+MHz20usGQj+DUfji+3Qw4o+aDux55FJHS5e/vz7Bhw3jyySdJSkpi1KhRjnWNGjXiu+++Y9WqVVSrVo0333yT+Pj4Ioeb3r1707hxY0aOHMlrr71GUlJSvhCTc4xDhw7xzTff0LFjRxYsWMDs2bPztalXrx4HDhxg06ZN1K5dm4CAgAKPgI8YMYLnnnuOkSNHMnHiRE6cOMH48eO5/fbbHbekSspms7Fp06Z8y7y8vOjduzctW7ZkxIgRvP3222RlZfHAAw/QvXt3OnTowLlz53j88ce56aabiI6O5siRI6xbt46hQ4cC8PDDD9OvXz8aN27MmTNn+PXXX2nWrNll1XopTg03P//8c76fp02bRmhoKOvXr6dbt24X3M5isVy0w5Xkslot1Ar2oVawD1c3CnEsT8+ysTM2mc2HE/jjwGl+23OChLOZzNt8jHmbj+FutdCjSQhD29XmmmaheLm7XeQol8HTD279Ej7uAUf+gJ/+AQPeLptjiUiVdvfdd/Ppp59y/fXX5+sf8/TTT7N//3769OmDr68v9913H4MHDyYxMbFI+7VarcyePZu7776bTp06Ua9ePd599918Lw8cOHAgjzzyCOPGjSM9PZ3+/fvzzDPPMHHiREeboUOHMmvWLHr27ElCQgJTp07NF8IAfH19WbhwIQ899BAdO3bE19eXoUOH8uabb17WuQHz8fi2bdvmW9agQQP27t3LDz/8wPjx4+nWrRtWq5W+ffvy3nvmsDpubm6cOnWKO+64g/j4eGrWrMmNN97I888/D5ihaezYsRw5coTAwED69u3LW2+9ddn1XozFMCrOy0b27t1Lo0aN2Lp1Ky1atCi0zbRp07jnnnuoVasWdruddu3a8corrxATE1No+/T0dNLT0x0/JyUlERUVRWJiYrE7crmyLJudTYcTWLb7BEt3HWdHbJJjXbCvBwNaRTKySz0ahpbR43t7foGvbgIMGPAOtB9VNscRkRJLS0vjwIEDREdH4+3t7exyxAVd7M9YUlISQUFBRfr9XWHCjd1uZ+DAgSQkJPD7779fsN3q1avZs2cPrVq1IjExkddff50VK1awfft2ateuXaD9xIkTHekxL4Wbi9sTn8z3G44ye+MR4pPMcGixQJ/m4TzQswGtageX/kF/ewOWvABWD7jzR4i69COYIlJ+FG6krLlcuBkzZgw//fQTv//+e6Eh5UIyMzNp1qwZw4cP58UXXyywXlduLo/NbrBq30k+X32QxTty30bZtWENxl/TiCvr1yi9gxmG2e9m51wIrAVjVoJPtdLbv4hcFoUbKWulFW4qxKPg48aNY/78+axYsaJYwQbMx93atm3L3r17C13v5eWlMTkug5vVwtWNQri6UQh74pP5cPk+5m46xsq9p1i59xR9YsJ46vrm1KlRCmOsWCwweDLEb4PT+2H+hOwhG6pOh3EREbl8Tn2zm2EYjBs3jtmzZ7N06VKio6OLvQ+bzcbWrVsdj99J2WkUFsCbt7Rh2eM9uP3KurhZLSzcHk/vN5fzn593kZKedfkH8QqAGz8xx6DaPgu2zrz8fYqISJXi1HAzduxYvvzyS6ZPn05AQABxcXHExcVx7lzuS+nuuOMOnnzyScfPL7zwAosWLWL//v1s2LCB2267jYMHD3LPPfc44ytUSbWr+fLi4Bb8/NDVXN2oJhk2O5OX7eOa15fx09bYUjhAB+jxT3N+waNw5uDF24tIuaogvRnEBZXWny2nhpsPP/yQxMREevToQUREhGP69ttvHW0OHTpEbGzuL8wzZ85w77330qxZM66//nqSkpJYtWpVkd9HIKWnUVgAn9/Vif/e0YG6NXw5npzOmK828OiMzSSnlXz0WQCumgC1O0F6EsweDfaLv9hKRMpezltvz5510mCZ4vJy3gqdd+iIkqgwHYrLS3E6JEnRpWfZeHfJHj5ctg+7AbWr+fDmLW3oFF1wSIoiO30AplwFGSnQ6zm4ekLpFSwiJRIbG0tCQgKhoaH4+jpxCBdxOXa7nWPHjuHh4UGdOnUK/NmqlE9LlReFm7L159+neWTGJg6fPofFAmO6N+DR65qUfFiHjV/BDw+A1R3uXQoRrUu3YBEpFsMwiIuLIyEhwdmliAuyWq1ER0fj6elZYJ3CzUUo3JS95LRMnp+3g+/WHwGgZ5MQ3vu/duaQEcVlGDDjdnP8qYg2ZsCxltHbkkWkyGw2G5mZl3n7WeQ8np6eWK2F95hRuLkIhZvyM2/zMR7/bjNpmXaahgfw6aiO1Ar2Kf6OkuPh/Y6Qngh9X4UrR5d+sSIiUqEV5/e3UzsUi2sb0DqSb+/rTEiAF7vikhn0/ko2HU4o/o4CwuDaieb80hch8WhplikiIi5G4UbKVOuoYH4Y25VmEYGcTEln2Eer+XlbCR4XbzfKfHoqIwV+eqLU6xQREdehcCNlLjLYh5mjO9OraSjpWXbGTt/I/C3HircTq9UcLdzqDrvmw64FZVKriIhUfgo3Ui78vdz5+I4ODG1XG5vd4KFvNjFvczEDTlgMdBlvzv/4OKQnl36hIiJS6SncSLlxs1r4z02tuKm9GXAe/rYEAafbExBcF5KOwq+vlE2hIiJSqSncSLlys1p4dehlBBxPX+j/pjm/9iM4sbtsChURkUpL4UbKXU7Aubl9zi2qjSzZGV/0HTTqDU2uB8MGi58tu0JFRKRSUrgRp8gJOEPb1cZuwLjpG9l6JLHoO7j2BbNz8V8/w/7lZVeoiIhUOgo34jRWq4V/D23J1Y1qci7Txl3/W8fRhHOX3hCgZiPocJc5v+gpsNvLrlAREalUFG7EqTzcrHwwoh1NwgI4kZzOXVPXkVTUEcW7/xO8AiFuK2z5pmwLFRGRSkPhRpwu0NuDz+7sSEiAF7vjkxn71QYybUW4EuNXA65+1Jxf8iJknC3bQkVEpFJQuJEKoVawD5+N7IiPhxu/7TnJc3O3F23DK0ZDUB1IPgarPyjbIkVEpFJQuJEKo2XtIN4b3haLBaavPcT32aOKX5SHN/R+zpz//S1zkE0REanSFG6kQundPIyHejUC4Ok52/grvghvIW4xFGp1gMxUWPGfMq5QREQqOoUbqXDGX9OIqxqaT1A98NUGUtOzLr6BxQK9J5rzGz6HxCJc8REREZelcCMVjpvVwtu3tiE0wIu9x1N4Zs42DMO4+EbRV0O9q8GWAb+9UT6FiohIhaRwIxVSTX8v3hveFqsFZm08yrfrDl96ox5Pmp8bvoCEQ2VboIiIVFgKN1JhXVG/Bo/1aQLAs3O3szM26eIb1OsK0d3AnqmrNyIiVZjCjVRoo7s1oGeTEDKy7Dw6Y/Ol33+Tc/Vm45dw5mDZFygiIhWOwo1UaFarhVdvakWwrwc7YpP44Ne9F9+gbheI7g72LF29ERGpohRupMILDfDmhUEtAHh/6V62Hb3EAJs9/2V+bvoKzvxdtsWJiEiFo3AjlcKAVhH0axFOlt3gsZmbSc+yXbhxnSuhfk/z6s2K18uvSBERqRAUbqRSsFgsvDi4BdX9PNkVl8x7Sy5xe8px9Wa6npwSEaliFG6k0qjp78WL2benPly+j82HEy7cOKqT+eSUYYM1H5ZPgSIiUiEo3Eil0r9VBDe0isBmN3j8u0s8PdXlIfNz/f/g3JnyKVBERJxO4UYqnRcGmben/opPYdrKvy/csGEvCI0xx5z687Nyq09ERJxL4UYqnep+nvyzb1MA3v7lL+KT0gpvaLFAl/Hm/NqPICu9nCoUERFnUriRSumm9rVpExVMaoaNlxfsvHDDFkMhIBJS4mHLjPIrUEREnEbhRiolq9XCi4NaYLHA3M3HWL3vVOEN3T3hyjHm/Kr3wH6JNxyLiEilp3AjlVbL2kH8X6c6ADw3d9uFOxe3HwVegXByN+xZVH4FioiIUyjcSKX2eJ8mVPP14K/4FP636u/CG3kHmgEHYNW75VWaiIg4icKNVGrBvp78w9G5eA/HL9S5+MoxYPWAgyvhyPpyrFBERMqbwo1Uerd0iKJ1VDAp6Vm8vmh34Y0CI6Hlzeb86vfKrzgRESl3CjdS6VmtFp69oTkA360/wt7jyYU37DzW/NwxFxKPllN1IiJS3hRuxCW0r1uNa5uHYTfg9YV/Fd4ovAXUvcockkEv9RMRcVkKN+IyHu/TBKsFft4ex8ZDFxhu4Yr7zc/1UyHzAv1zRESkUlO4EZfROCyAG9vVBuDVn3dhGEbBRk2uh6AoOHsKtn1fzhWKiEh5ULgRl/LItY3xdLOyZv9pVuw5WbCBmzt0vNucXzsFCgtAIiJSqSnciEupFezD7Z3rAvDqT7uw2wsJL+1Ggrs3xG2BQ2vKuUIRESlrCjficsb2bEiAlzs7YpOYvzW2YAPf6tDqFnN+7ZTyLU5ERMqcwo24nOp+ntzXrT4AbyzaXfiwDJ2yOxbvnAeJR8qxOhERKWsKN+KS7roqmpr+nhw8dZa5m44VbKDHwkVEXJbCjbgkPy937rnavHrzwbK92Arre5PzWPifUyHzXDlWJyIiZUnhRlzWbVfWJcjHg/0nUvlpWyF9b3IeCz932nxrsYiIuASFG3FZ/l7u3Nm1HgDvL91b8MkpN3dod4c5v35q+RYnIiJlRuFGXNqoLvXw93JnV1wyS3YdL9ig7W1gcYNDq+H4rvIvUERESp3Cjbi0YF9Px3tv3v91b8G3FgdGQuO+5vz6aeVbnIiIlAmFG3F5d18VjbeHlc2HE/h9byFvLe5wp/m5+Wt1LBYRcQEKN+Lyavp78X+dzKs37y3dW7BBg2vMjsVpCbDjh/ItTkRESp3CjVQJ93Wrj6eblT8OnGbt/lP5V1rdzCEZQLemRERcgMKNVAnhQd7c1MEcMfzD5fsKNsjXsXhnOVcnIiKlSeFGqoz7rq6PxQLLdp9gT3xy/pWBEdCknzm//n/lX5yIiJQahRupMurV9OO65mEA/Pe3AwUbtB9lfm6ero7FIiKVmFPDzaRJk+jYsSMBAQGEhoYyePBgdu/efcntZs6cSdOmTfH29qZly5b8+OOP5VCtuIJ7s4dkmL3xKCeS0/OvbHANBNWBtER1LBYRqcScGm6WL1/O2LFjWbNmDYsXLyYzM5PrrruO1NTUC26zatUqhg8fzt13383GjRsZPHgwgwcPZtu2beVYuVRW7etWo22dYDJsdr5Y/Xf+lVa3PG8s1q0pEZHKymIUeKuZ85w4cYLQ0FCWL19Ot27dCm0zbNgwUlNTmT9/vmPZlVdeSZs2bZgyZcolj5GUlERQUBCJiYkEBgaWWu1Sefy4NZYHvtpANV8PVv2zFz6ebrkrk47BWzFg2GH8BqjRwHmFioiIQ3F+f1eoPjeJiYkAVK9e/YJtVq9eTe/evfMt69OnD6tXry60fXp6OklJSfkmqdr6xIQTVd2HM2cz+X7DkfwrAyOhfk9zfvPX5V+ciIhctgoTbux2Ow8//DBdu3alRYsWF2wXFxdHWFhYvmVhYWHExcUV2n7SpEkEBQU5pqioqFKtWyofN6uFu7pGA/DZ7wcKDqjZdoT5uelrsNvKuToREblcFSbcjB07lm3btvHNN9+U6n6ffPJJEhMTHdPhw4dLdf9SOd3SIYpAb3f2n0wtOKBmk/7gHQRJR+DAcucUKCIiJVYhws24ceOYP38+v/76K7Vr175o2/DwcOLj4/Mti4+PJzw8vND2Xl5eBAYG5ptE/Lzc+b8rzCEZPlmxP/9KD29ocZM5v2l6OVcmIiKXy6nhxjAMxo0bx+zZs1m6dCnR0dGX3KZz584sWbIk37LFixfTuXPnsipTXNSoLvVwt1r44+/TbD2SmH9lzq2pnfPgXEK51yYiIiXn1HAzduxYvvzyS6ZPn05AQABxcXHExcVx7lzuC9TuuOMOnnzyScfPDz30ED///DNvvPEGu3btYuLEifz555+MGzfOGV9BKrHwIG/6t4oA4PPzHwuPbAchTSErDbbPLv/iRESkxJwabj788EMSExPp0aMHERERjunbb791tDl06BCxsbGOn7t06cL06dP5+OOPad26Nd999x1z5sy5aCdkkQu5o3M9AOZuPsaZ1IzcFRYLtMnpWPxV+RcmIiIlVqHec1Me9J4bycswDG5473e2H0viyX5Nub97nvfaJMfDm83AsMHYPyCkifMKFRGp4irte25EypvFYmFk9tWbL9cexJb3sfCAMGh0rTmvjsUiIpWGwo1UeQNaRxLk48Hh0+dYtvu8x8Jzbk1t/gZsWeVfnIiIFJvCjVR5Pp5u3NLBfAXB56sP5l/ZuC/4VIeUONj/qxOqExGR4lK4EQFuu7IuFgss/+sEB07mGbjV3RNaDDXnt8xwTnEiIlIsCjciQN0afvRoHALAl2vOu3rTapj5uWsBZFx4xHoREakYFG5Est3RpR4AM/88zNmMPP1raneAatGQmQq7fnROcSIiUmQKNyLZujcKoW4NX5LSsvhh07HcFRYLtLzZnN+qW1MiIhWdwo1INqvVwm3Z400VvDV1i/m5dwmkniznykREpDgUbkTyuKl9bTzdrGw/lsS2o3nGm6rZCCLamC/003AMIiIVmsKNSB7V/Dzp08IcYf7rPw7lX5lz9WbLt4iISMWlcCNynuEdowCYu+lY/o7FLYaCxQpH1sHp/U6qTkRELkXhRuQ8V9avQd0aviSnZ7FgS+6grQSEQ3R3c37rd84pTkRELknhRuQ8VquFWzqYV2++WXc4/0rHrakZULXGnBURqTQUbkQKcXP72rhZLaw/eIY98cm5K5reAO7ecGoPxG5yWn0iInJhCjcihQgN9OaapqHAeVdvvAOhST9zfstMJ1QmIiKXonAjcgHDO5m3pmZtOEJ6li13RcvsW1PbvgO7rZAtRUTEmRRuRC6ge+NQwgO9OXM2k0Xb43NXNOwN3sGQEg9//+60+kREpHAKNyIX4Ga1cEuH2gB8m/fWlLsnNB9ozm/TU1MiIhWNwo3IRdzcIQqLBX7fe5JDp87mrmhxk/m5Yy5kZTinOBERKZTCjchFRFX35aqGNQH4bsOR3BX1rgL/cEhLgH1LnFOciIgUSuFG5BJuam/empq14Qh2e/a7baxuEDPEnNcL/UREKhSFG5FL6BMTToCXO0fOnGPtgdO5K1pm35ra/SNkpDqnOBERKUDhRuQSvD3cuKF1BADf5701Vas9VKsHmWdh90/OKU5ERApQuBEpgqHtzFtTP26NJTU9ezBNi8UcTBNg2/dOqkxERM6ncCNSBO3rVqNeDV/OZtj4eVtc7oqcp6b2LIZzZ5xTnIiI5KNwI1IEFovFcfXmu/V5bk2FNYfQ5mDPhJ3znFSdiIjkpXAjUkRD2tUCYPX+Uxw5k/edN9m3pvTUlIhIhaBwI1JEtav50qVBDQBmbTiauyIn3Pz9GyTHF7KliIiUJ4UbkWLIuTX1/YYjGEb2O2+qR0OtDmDYYccc5xUnIiKAwo1IsfRrGY6fpxsHT53lz4N5OhDnXL3ZPts5hYmIiIPCjUgx+Hq606+l+c6b7/7M07G4+SDz89BqSDrmhMpERCSHwo1IMeUMx/Dj1ljSMm3mwqBaEHWlOb/jBydVJiIioHAjUmyd6lUnMsib5PQslu46nrsiZ6wp3ZoSEXEqhRuRYrJaLQxqaz4WPntjnqemmg8ELHB4LSQeKXxjEREpcwo3IiUwJDvcLNt9nDOpGebCwEio09mc160pERGnUbgRKYHGYQE0jwgk02awYGts7grdmhIRcTqFG5ESyrl6MyffralBgAWOrIOEQ84pTESkilO4ESmhgW0isVjgz4NnOHw6eziGgDCod5U5r1tTIiJOoXAjUkJhgd50bVATgB825bl6EzPY/NStKRERp1C4EbkMg9pEAuZTU47hGJoNBIsVjq6HMwedWJ2ISNWkcCNyGfq2CMfL3cq+E6lsO5pkLvQPzb01pas3IiLlTuFG5DIEeHtwbfMw4Lx33jiemprlhKpERKo2hRuRy5Tz1NTczcfIstnNhc0GgsUNYjfD6f1OrE5EpOpxL+mG3333HTNmzODQoUNkZGTkW7dhw4bLLkyksujWOIRqvh6cTEln5b5TdG8cAn41Ibob7P8Vts+Bqyc4u0wRkSqjRFdu3n33Xe68807CwsLYuHEjnTp1okaNGuzfv59+/fqVdo0iFZqHm5UbWpkdi+duyjMiuF7oJyLiFCUKN5MnT+bjjz/mvffew9PTkyeeeILFixfz4IMPkpiYWNo1ilR4A7Ofmlq0PS53pPBmA8xbU3Fb4NQ+J1YnIlK1lCjcHDp0iC5dugDg4+NDcnIyALfffjtff/116VUnUkm0r1ONiOyRwpftPmEu9K0O9XuY87p6IyJSbkoUbsLDwzl9+jQAderUYc2aNQAcOHAg910fIlWI1WphQGvz6s28zbo1JSLiTCUKN9dccw1z584F4M477+SRRx7h2muvZdiwYQwZMqRUCxSpLAZk97tZsiuelPQsc2HT/mB1h/htcOIvJ1YnIlJ1lOhpqY8//hi73XzkdezYsdSoUYNVq1YxcOBA7r///lItUKSyaFErkOiafhw4mcovO+IZ3LZW9q2pnrB3MeyYA92fcHaZIiIur0RXbqxWK+7uubno1ltv5d1332X8+PF4enqWWnEilYnFoltTIiIVQZGv3GzZsqXIO23VqlWJihGp7Aa2juDdJXtYsecECWczCPb1hKbXwzwPOL4Dju+C0KbOLlNExKUVOdy0adMGi8WCYRhYLJaLtrXZbJddmEhl1DA0gGYRgeyMTeLnbXHc2qkO+FSDBtfAnoXmranQfzq7TBERl1bk21IHDhxg//79HDhwgO+//57o6GgmT57Mxo0b2bhxI5MnT6ZBgwZ8//33ZVmvSIU3oHUEYA7H4NDiRvNTt6ZERMpcka/c1K1b1zF/88038+6773L99dc7lrVq1YqoqCieeeYZBg8eXKpFilQmA1pF8p+fd7N6/ymOJ6URGugNTfqBmyec2AXxOyCsubPLFBFxWSXqULx161aio6MLLI+OjmbHjh1F3s+KFSsYMGAAkZGRWCwW5syZc9H2y5Ytw2KxFJji4uKK+xVEykxUdV/a1QnGMGDB1lhzoXcQNOxtzuvqjYhImSpRuGnWrBmTJk3KN2BmRkYGkyZNolmzZkXeT2pqKq1bt+aDDz4o1vF3795NbGysYwoNDS3W9iJlLeepqXy3ppoPNj93/FD+BYmIVCEles/NlClTGDBgALVr13Y8GbVlyxYsFgvz5s0r8n769etXooE2Q0NDCQ4OLvZ2IuWlf6sIXpy/g42HEjh8+ixR1X2hSV/z1tTJ3XB8J4QW/R8CIiJSdCW6ctOpUyf279/PSy+9RKtWrWjVqhUvv/wy+/fvp1OnTqVdYwFt2rQhIiKCa6+9lpUrV160bXp6OklJSfkmkbIWGuDNlfVrAOfdmmpwjTm/fY5zChMRqQJKdOUGwM/Pj/vuu680a7mkiIgIpkyZQocOHUhPT+e///0vPXr0YO3atbRr167QbSZNmsTzzz9frnWKgHlratW+U8zbfIzR3RuYC5sPhr9+Nm9N9XzSqfWJiLgqi1HEkS7nzp1Lv3798PDwcIwrdSEDBw4sfiEWC7Nnzy72k1bdu3enTp06fPHFF4WuT09PJz093fFzUlISUVFRJCYmEhgYWOw6RYrqTGoGHV/+hSy7wZJHu9MgxB/OJcBrDcGeCWP/gJAmzi5TRKRSSEpKIigoqEi/v4t85Wbw4MHExcURGhp60QBisVjK9SV+nTp14vfff7/gei8vL7y8vMqtHpEc1fw8uapRTZbtPsH8zbE81LsR+ARDg56wZ5F5a6rHP5xdpoiIyylynxu73e54Kslut19wKu+3E2/atImIiIhyPaZIUeWMFD5381EcF0n11JSISJkqcZ+b0pCSksLevXsdPx84cIBNmzZRvXp16tSpw5NPPsnRo0f5/PPPAXj77beJjo4mJiaGtLQ0/vvf/7J06VIWLVrkrK8gclHXxoThOdvKvhOp7IpLpllEYPZYU+5wfDuc3AM1Gzm7TBERl1LkcPPuu+8WeacPPvhgkdr9+eef9OzZ0/HzhAkTABg5ciTTpk0jNjaWQ4cOOdZnZGTw6KOPcvToUXx9fWnVqhW//PJLvn2IVCSB3h70bBLCwu3xzN9yzAw3PtWgfg/Y+4t5a6r7484uU0TEpRS5Q/H5byQ+ceIEZ8+edbxvJiEhAV9fX0JDQ9m/f3+pF1paitMhSaQ0zNt8jPFfb6ROdV+WP97DHHh2wxcwdxyEtYQxF+4zJiIipuL8/i7WwJk508svv0ybNm3YuXMnp0+f5vTp0+zcuZN27drx4osvXvYXEHElvZqF4uPhxqHTZ9lyJNFc2LQ/WN0hfiuc2ufcAkVEXEyJXuL3zDPP8N5779GkSe5jrE2aNOGtt97i6aefLrXiRFyBr6c7vZuHAeZVHHNhdYjuZs5rrCkRkVJVonATGxtLVlZWgeU2m434+PjLLkrE1QxoZT7RN39LLHa7npoSESlLJQo3vXr14v7772fDhg2OZevXr2fMmDH07t271IoTcRXdm4QQ4O1OXFIafx48Yy5segNY3CBui25NiYiUohKFm88++4zw8HA6dOjgeElep06dCAsL47///W9p1yhS6Xm5u3Fd83Agz60pvxoQfbU5r6s3IiKlptjhxjAMzp07x/fff8/u3buZOXMmM2fOZOfOnfz444+OF/2JSH43tDZvTf20LZYsm91c6Lg1NccpNYmIuKJiv8TPMAwaNmzI9u3badSoEY0a6QVkIkVxVcOaBPt6cDIlgz8OnKZLw5rQbAAseBRiN8PpA1A9+tI7EhGRiyr2lRur1UqjRo04depUWdQj4rI83Kz0jcm+NbUl1lzoVxPqXWXO6+qNiEipKFGfm3//+988/vjjbNu2rbTrEXFpN2SPNfXztlgyc25NxQw2P7fPcUpNIiKupkTh5o477uCPP/6gdevW+Pj4UL169XyTiBTuyvrVqeHnyZmzmazal331s+kAsFghdhOc+duZ5YmIuIQSDZz59ttvl3IZIlWDu5uVfi3D+XLNIeZvPkb3xiHgH2LemjqwwnxqqutDzi5TRKRSK1G4GTlyZGnXIVJl3NAqki/XHGLh9jheHtIST3er+dTUgRXmrSmFGxGRy1Ki21IA+/bt4+mnn2b48OEcP34cgJ9++ont27eXWnEirqhjveqEBniRlJbFb3tOmAubDTRvTR3bAGcOOrdAEZFKrkjhZvfu3fl+Xr58OS1btmTt2rXMmjWLlJQUADZv3sxzzz1X+lWKuBA3q4XrW+YOxwCYt6bqdjXn9UI/EZHLUqRwM2vWLEaMGIHNZgPgn//8Jy+99BKLFy/G09PT0e6aa65hzZo1ZVOpiAu5IXusqcU74knLNP+7cjw1pUfCRUQuS5HCzWOPPUb16tXp06cPAFu3bmXIkCEF2oWGhnLy5MnSrVDEBbWrU42IIG9S0rNYtjvPrSkscHQ9JBxyan0iIpVZkcKNh4cH7733Hvfffz8AwcHBxMbGFmi3ceNGatWqVboVirggq9VCf8etqeyxpvxDdWtKRKQUFKtD8c033wzArbfeyj/+8Q/i4uKwWCzY7XZWrlzJY489xh133FEmhYq4mhtamy/0W7LzOGczssyFjhf6zXZOUSIiLqBET0u98sorNGvWjDp16pCSkkLz5s3p1q0bXbp04emnny7tGkVcUuvaQURV9+Fcpo1fd2Xfmmo+yHxq6uh6vdBPRKSEihVubDYbr776Kj179mTjxo3cfvvtzJ8/ny+//JJdu3bxxRdf4ObmVla1irgUi8VC/5bm1Zt8t6ZyxprS1RsRkRIpVrh55ZVX+Ne//oW/vz+1atVi+vTpfPfdd9xyyy0aHVykBHKemlq66zgp6Tm3pm40P7fNclJVIiKVW7HCzeeff87kyZNZuHAhc+bMYd68eXz11VfY7fayqk/EpcVEBlKvhi/pWXaW7Iw3FzYbCBY3iNsCp/Y5t0ARkUqoWOHm0KFDXH/99Y6fe/fujcVi4dixY6VemEhVYLFYHCOFO17o51cD6vcw57fr6o2ISHEVK9xkZWXh7e2db5mHhweZmZmlWpRIVXJDa/PW1PLdJ0hKy/5vqUXOrSn1uxERKa5iDZxpGAajRo3Cy8vLsSwtLY3Ro0fj5+fnWDZrlv61KVJUTcICaBjqz97jKSzeHs/Q9rWhaX+Y9zAc3w4ndkNIE2eXKSJSaRTrys3IkSMJDQ0lKCjIMd12221ERkbmWyYiRWc+NWVevVmwNfvWlE81aHCNOa+OxSIixVKsKzdTp04tqzpEqrQBrSN4Z8kefttzgsSzmQT5epi3pvYsNPvd9PgnWCzOLlNEpFIo0Uv8RKR0NQwNoGl4AJk2g4Xb48yFTa4HNy84+RfEb3dugSIilYjCjUgFkfPOm3k5L/TzDoRG15rzempKRKTIFG5EKoj+2Y+Er9p3itOpGebCmCHm57ZZYBhOqkxEpHJRuBGpIKJr+hETGYjNbvDztuxbU437grsPnDkAxzY6t0ARkUpC4UakAsl9oV/2rSkvf2jSz5zf+p2TqhIRqVwUbkQqkJx+N6v3n+J4Upq5sOXN5ue278Fuc1JlIiKVh8KNSAUSVd2XtnWCMYw877xp2Bu8gyElDv7+zan1iYhUBgo3IhXMgOxbU/M2Z9+acveEmMHm/NaZzilKRKQSUbgRqWBuaBWBxQIbDiVw+PRZc2HOrakdcyEzzXnFiYhUAgo3IhVMaKA3V0bXAPLcmqrTBQJrQXoS7FnkxOpERCo+hRuRCmhA6/NuTVmt0GKoOb91hpOqEhGpHBRuRCqgvi3Ccbda2H4siX0nUsyFrW4xP/9aBOcSnFabiEhFp3AjUgFV9/PkqkY1AZi/OfvWVFgLCGkKtnTYOc+J1YmIVGwKNyIVVM5TU3M3H8UwDHNU8JyOxXpqSkTkghRuRCqo62LC8HS3su9EKrviks2FLW8yPw+sgOQ45xUnIlKBKdyIVFAB3h5c0yQUgLk5HYur1YOoKwDDfGOxiIgUoHAjUoHlfWrKyBkVPOfW1JZvnVSViEjFpnAjUoFd0zQUX083jpw5x8bDCebCmBvB6g6xm+H4TqfWJyJSESnciFRgPp5u9IkJB+CHjUfNhX41oFEfc37zN06qTESk4lK4EangBrXJvjW1JZZMm91c2HqY+bllhkYKFxE5j8KNSAV3VcOa1PT35HRqBr/vOWkubNwXvIMg+ZhGChcROY/CjUgF5+5m5Ybsd97M2ZR9a8rdK3c4Bt2aEhHJR+FGpBIY3LYWAIu2x5OanmUubHWr+bljLmSkOqkyEZGKR+FGpBJoXTuIejV8OZdpY9GO7Jf3RXWCatGQmQo75zu3QBGRCkThRqQSsFgsjqs3czYey1kIrbOv3mz+2kmViYhUPAo3IpXE4DZmuPltzwlOJKebC1tlPzW1fxkkHXNOYSIiFYzCjUglUa+mH62jgrEbMH9LdpCpHg11OgOGBtMUEcmmcCNSiQxpk/PUVJ6rNDlXbzZ9DTlDNIiIVGFODTcrVqxgwIABREZGYrFYmDNnziW3WbZsGe3atcPLy4uGDRsybdq0Mq9TpKK4oXUkblYLmw8ncOBk9hNSMYPBzQtO7DSHZBARqeKcGm5SU1Np3bo1H3zwQZHaHzhwgP79+9OzZ082bdrEww8/zD333MPChQvLuFKRiqGmvxdXNawJwJyc4Rh8qkHT6835jV86qTIRkYrDqeGmX79+vPTSSwwZMqRI7adMmUJ0dDRvvPEGzZo1Y9y4cdx000289dZbZVypSMUxJOepqU1Hc0cKb3eH+bl1BmSec1JlIiIVQ6Xqc7N69Wp69+6db1mfPn1YvXq1kyoSKX/XxYTh5+nGwVNn+fPgGXNhdA8IqgNpibBznjPLExFxukoVbuLi4ggLC8u3LCwsjKSkJM6dK/xfq+np6SQlJeWbRCozX093rm8ZAcD364+YC61WaHubOb/hcydVJiJSMVSqcFMSkyZNIigoyDFFRUU5uySRyza0fW0AFmyJJS0ze1TwNv8HWMyBNE/tc15xIiJOVqnCTXh4OPHx8fmWxcfHExgYiI+PT6HbPPnkkyQmJjqmw4cPl0epImWqU73q1Ar2ITk9i4Xbs4djCI6Chr3MeXUsFpEqrFKFm86dO7NkyZJ8yxYvXkznzp0vuI2XlxeBgYH5JpHKzmq1MLSd2bH4+w1Hc1fkdCzeNB1sWU6oTETE+ZwablJSUti0aRObNm0CzEe9N23axKFDhwDzqssdd9zhaD969Gj279/PE088wa5du5g8eTIzZszgkUcecUb5Ik51Yzvz1tTve04Qn5RmLmzcD3xrQkoc7F3sxOpERJzHqeHmzz//pG3btrRt2xaACRMm0LZtW5599lkAYmNjHUEHIDo6mgULFrB48WJat27NG2+8wX//+1/69OnjlPpFnKleTT861K2G3YDZOe+8cffMHUxTHYtFpIqyGEbVel97UlISQUFBJCYm6haVVHpf/3GIJ2dtpVGoP4se6YbFYoETu+GDTmBxgwk7ICDc2WWKiFy24vz+rlR9bkQkv/6tIvByt7LneApbjyaaC0OaQNQVYNjMvjciIlWMwo1IJRbo7cF1MeaVGcc7byC3Y/GG/4Hd7oTKREScR+FGpJLLeWpq7uZjZGRlB5mYIeAVBGf+hn1LLryxiIgLUrgRqeSubhRCaIAXZ85msnRX9nugPP2g7Qhz/o9PnFeciIgTKNyIVHJuVovjsfBv1+V5SWXHe8zPPYvMKzgiIlWEwo2ICxjW0RxWZPlfJziWkD3OWo0G0OAawIB1nzqvOBGRcqZwI+IComv6cUV0dewGfJe3Y3HHe83PjV9AZuGDy4qIuBqFGxEXcWsn8+rNjD8PY7dnv76qcR8IqgPnzsC2WU6sTkSk/CjciLiIfi0iCPB258iZc6zad8pcaHWDDnea8+vUsVhEqgaFGxEX4e3hxuA25mPh36zLHbaEdneAmycc2whH1jupOhGR8qNwI+JCcjoWL9oez5nUDHOhX02IudGc19UbEakCFG5EXEiLWkHERAaSYbPnDqYJ0Ok+83PbLEg95ZziRETKicKNiIu5NfvqzbfrDuMYF7d2e4hsC7Z0WP+ZE6sTESl7CjciLmZgm1p4uVvZHZ/M5iOJuSuufMD8XPsxZKU7pzgRkXKgcCPiYoJ8POjfMgKAb/N2LI4ZAoG1IPU4bJnhpOpERMqewo2IC8rpWPzDpmMkpWWaC9084Mox5vyq9zRauIi4LIUbERfUKbo6jcP8OZth47s/87yxuN1I8AqEk7th7y/OK1BEpAwp3Ii4IIvFwh2d6wHwxZqDuW8s9g6E9iPN+VXvOqc4EZEypnAj4qKGtK1FgJc7B06m8tvek7krrhgNVnf4+zfzxX4iIi5G4UbERfl5uXNTh9oAfL7q79wVQbVzX+q36v3yL0xEpIwp3Ii4sNuvrAvA0t3HOXTqbO6KLuPMz+2zIeFQIVuKiFReCjciLqx+iD/dGodgGPDl2oO5KyJaQ3R3MGywZorzChQRKQMKNyIubmRn8+rNt+sOcy7Dlruiy4Pm5/ppGpJBRFyKwo2Ii+vRJJSo6j4knstk7uY840017GVewclM1ZNTIuJSFG5EXJyb1cIdV9YD4H+rDuaON2WxQI9/mfN/fAwpJ5xToIhIKVO4EakCbu5QG28PKztik1j395ncFY37QGQ7yDwLK992Wn0iIqVJ4UakCgj29WRI21oAfPLb/twVFgv0zL56s+5TSI53QnUiIqVL4Uakirjn6vpYLLB4Rzx7j6fkrmjYG2p3hKxzunojIi5B4UakimgQ4k/vZmEA/PdiV2+SYp1QnYhI6VG4EalCRnevD8CsDUc5npSWu6J+T4i6Emzp8PubTqpORKR0KNyIVCHt61anQ91qZNjsTM07JEPeqzfrp0Hi0cI2FxGpFBRuRKqY+7s3AODLNQdJTsvMXRHdDepeBbYMWP5vJ1UnInL5FG5EqpheTUNpEOJHcloW3/xxOHeFxQK9njXnN34J8TucU6CIyGVSuBGpYqxWC/d1M/vefLbyABlZ9tyVda6AZgPBsMPiZ51UoYjI5VG4EamCBretRUiAF7GJaczbfCz/yt4TweoOexfDvl+dUp+IyOVQuBGpgrzc3birazQAH63Yh91u5K6s0QA63mPOL34G7PZC9iAiUnEp3IhUUf93RR0CvN35Kz6Fn7fH5V/Z7QnwCoS4rbDlW+cUKCJSQgo3IlVUkI+H4+rNO7/syX/1xq8GXP2oOb/0Rcg854QKRURKRuFGpAq766poArzd2R2fXPDqzRWjISgKko7CmsnOKVBEpAQUbkSqsItevfHwhmueMed/e0vDMohIpaFwI1LFXfTqTcuboVZ7yEiGhU86p0ARkWJSuBGp4i569cZqhRveAosVts+GPb84qUoRkaJTuBGRfFdvftp23tWbiNZwxRhzfsEEyDhb/gWKiBSDwo2IEOTjwd1XZV+9WfJX/qs3YA6qGVgLEg7CitecUKGISNEp3IgIAHd2jXa892bB1vM6D3v5Q7//mPOr3oXjO8u/QBGRIlK4ERHAvHpz79XmmFOv/ryLtExb/gbNboAm14M9C+ZP0JuLRaTCUrgREYd7ro4mPNCbI2fO8dnKAwUb9HsVPHzh0CrY+Hn5FygiUgQKNyLi4Ovpzj/6NQHgg6V7OZ6clr9BcB2z/w3Az/+Ck3vLuUIRkUtTuBGRfAa1rkXrqGBSM2y8sfCvgg2ufADqXQ2ZqfDdnZCVXv5FiohchMKNiORjtVp49obmAMxYf5htRxPPa+AGN34MPtUhbgssecEJVYqIXJjCjYgU0L5uNQa1icQw4IX5OzCM8x4ND4yEQR+Y86vf18v9RKRCUbgRkUL9o29TvD2s/HHgND+f/2I/gKbXQ6f7zPk5oyE5vnwLFBG5AIUbESlUZLAP93VrAMDLP+7kXIatYKNrX4TQGEg9YQYcPR4uIhWAwo2IXNDo7vWJCDIfDX936Z6CDTy84abPwN0H9i2F5f8u/yJFRM6jcCMiF+Tr6c7zA2MA+GTFfnbGJhVsFNrUHFwTYPmrsHNeOVYoIlKQwo2IXNR1MeH0jQkny27w5Kyt2M4fdwqgzfDcwTVnj4bju8q3SBGRPCpEuPnggw+oV68e3t7eXHHFFfzxxx8XbDtt2jQsFku+ydvbuxyrFal6Jg6MIcDLnU2HE/hyzcHCG133ovn+m4wU+Ob/4FxCudYoIpLD6eHm22+/ZcKECTz33HNs2LCB1q1b06dPH44fP37BbQIDA4mNjXVMBw9e4C9bESkV4UHePNGvKQD/+XkXxxLOFWzk5gE3T4OgKDi9D2bdC/ZCOiGLiJQxp4ebN998k3vvvZc777yT5s2bM2XKFHx9ffnss88uuI3FYiE8PNwxhYWFlWPFIlXTiE51aFfHfHPxc3O3F97IryYM+xLcvWHPIvhlYrnWKCICTg43GRkZrF+/nt69ezuWWa1WevfuzerVqy+4XUpKCnXr1iUqKopBgwaxffsF/qIF0tPTSUpKyjeJSPFZrRYm3dgKd6uFxTvi+WlrbOENI9vAwPfM+VXvwqr3yq1GERFwcrg5efIkNputwJWXsLAw4uIKeWkY0KRJEz777DN++OEHvvzyS+x2O126dOHIkSOFtp80aRJBQUGOKSoqqtS/h0hV0SQ8gNHdzXffPDl7K3GJaYU3bHUL9HrOnF/0NGz8spwqFBGpALeliqtz587ccccdtGnThu7duzNr1ixCQkL46KOPCm3/5JNPkpiY6JgOHz5czhWLuJYHezWiRa1AEs5mMmHGpsKfngK46hHoMt6cnzseds4vvyJFpEpzaripWbMmbm5uxMfnf217fHw84eHhRdqHh4cHbdu2Ze/evYWu9/LyIjAwMN8kIiXn6W7lnVvb4uPhxqp9p/h4xf7CG1os5huM294Ght0cQfzAivItVkSqJKeGG09PT9q3b8+SJUscy+x2O0uWLKFz585F2ofNZmPr1q1ERESUVZkicp4GIf6Ol/u9sWg3mw4nFN7QYoEb3oGmN4AtA74eDvuXl1+hIlIlOf221IQJE/jkk0/43//+x86dOxkzZgypqanceeedANxxxx08+eSTjvYvvPACixYtYv/+/WzYsIHbbruNgwcPcs899zjrK4hUSTd3qE3/lhFk2Q0e+mYjKelZhTd0c4ehn0J0d/MdOF8Ohc3flG+xIlKluDu7gGHDhnHixAmeffZZ4uLiaNOmDT///LOjk/GhQ4ewWnMz2JkzZ7j33nuJi4ujWrVqtG/fnlWrVtG8eXNnfQWRKslisfDKkJZsOpzAwVNnefaHbbx5S5vCG3t4w//NgB8egG3fw+z7IeEQdHvcvLojIlKKLIZhXKA3oGtKSkoiKCiIxMRE9b8RKQXr/j7NsI9WYzfghUEx3NG53oUb2+2w5HlY+bb5c9vb4Ia3zRcAiohcRHF+fzv9tpSIVG4d61Xn8T7m24ufn7eD3/acuHBjqxWufR76vwEWq/mI+Jc3QurJcqpWRKoChRsRuWyju9fnxna1sNkNHvhqA3uPp1x8g473wK1fg4ef+QTVR93g8LryKVZEXJ7CjYhcNovFwqQbW9KhbjWS07K4+3/rOJOacfGNmvSFe5dAjUaQdBSm9oM/PoGqdadcRMqAwo2IlAovdzc+ur09tav5cPDUWUZ/uZ6MLPvFNwptBvcuheaDwJ4JPz4Gs+6DNA2TIiIlp3AjIqWmhr8Xn47siL+XO2sPnOaf32/BfqE3GOfwDoSb/wfXvQwWN9g6Az64AnYtKJ+iRcTlKNyISKlqEh7Ae8Pb4ma1MGvjUZ75YRuXfCjTYoEu42DUAqgWDcnH4Jv/g29vh6QLDNApInIBCjciUup6Ng3lzVtaY7HAV2sP8eL8nZcOOAB1O8MDq81xqSxusHOueRXnj0/Alln2hYuIS1C4EZEyMahNLV4d2gqAz1Ye4PVFu4u2oYcP9J4I9y+HyHaQnmj2xfmgE2ybpQ7HInJJCjciUmZu6RDFi4PMMag++HUf7y3ZU/SNw1vCPb/A9a+Db004vd8cfPOTnrB/WdkULCIuQeFGRMrU7Z3r8XT/ZgC8sfgvXpy/A9ulOhnnsLpBp3vhoU3Q40nw9IdjG+HzQfBZX/hroa7kiEgBGn5BRMrFR8v3MemnXQD0bhbGO7e2wc+rmMPbpZyAFf+B9dPMUcYBQmPgqoch5kZzkE4RcUnF+f2tcCMi5Wbu5mM8NnMzGVl2YiID+XRkR8KDvIu/o6RYWDMZ/vzMHGkcICAS2vyfOdVoULqFi4jTKdxchMKNiHOtP3iG+z7/k1OpGYQHevPfkR1oUSuoZDs7dwbWfQprp0BqnjGt6nSBtiOgaX/wqVY6hYuIUyncXITCjYjzHTp1lrv+t469x1PwdLPyj35NubNLPaxWS8l2mJUOu380B+LctxSM7DcjW9wg6gpofB00ug5Cm5vv1BGRSkfh5iIUbkQqhsRzmTw6YxO/7DwOQPfGIbx+c2tCArwub8dJx2Dz17BlBpzYlX9dYG1o0BMa9oL6PXRVR6QSUbi5CIUbkYrDMAy+XHOQlxbsJD3LTg0/T167uRXXNA0rnQOcOQh7FpnTgRWQlZa7zmKFWu2h3lVQpzNEdVLYEanAFG4uQuFGpOL5Kz6ZB7/eyK64ZAAGtYnkX9c3IyywBJ2NLyTjLBxcBfuWwN4lcLKQlwqGNIXaHc137IQ2h7AY8K1eejWISIkp3FyEwo1IxZSWaeO1hbv5bOUBDAP8PN14uHdjRnWth4dbGbySK/EI7PsVDq2Bw2vg1N7C2wVEmKOX12wCIU3MABTSRKFHpJwp3FyEwo1Ixbb1SCLP/LCNTYcTAGgU6s/TNzSnW6OaWMqyM3DqSTi81nxJYPwOiN8GCQcv3N472HzkvHp9cwqsBf6h4BcK/iHgF2IOJSEipULh5iIUbkQqPrvd4Lv1R/j3z7s4nWq+rK9NVDAP9mpIzyahZRty8kpPhuO7zI7JJ3bBid3mlHioaNt7B4F/OASE5X76hYJ/WHYACjXbeAeCZwBY9dJ4kQtRuLkIhRuRyiPxbCbvLNnDV2sPkp5lPt4dExnI+Gsacm3zcNxK+uj45cpIhTN/m+NdndpnfibHmu/aSTkBqcdz36BcHJ4BZtDxCgCvnM/zJk9/8PIHDz/w9ANPX3OZu3f25GV+eniDuw+4eejxd3EJCjcXoXAjUvmcSE7nv7/t54s1BzmbYQOgVrAPt3aM4paOUaXb8bg0GAakJUJKPCTHmVNKHKQcz57is4PQcUhPKlkQKiqLFTx8swOPD7h5Zoeg7E/Hz17m5OZlrnPzMoORuxdYPcx5Nw+zveMzp62n+U4hi9W8+mSxApbsUJUdrCyWPPvJ3sbNHaw5k4c5llje9QplkofCzUUo3IhUXmdSM/hs5QE+X32QxHOZALhZLfRqGsqwjlFc3SgET/dKeGsnM80MOWlJkJ4I6Snmz+nJ5rKMZHNZRkruZ0aqOWWeNdtlpZuPuud84gJ/teeEobxhyWIxg5Q1bzByyxOYssOR1a2Q/Z3XzuqRu73VLXu/2SHNYs0ObJb8YczNMzeIWbPrKLBdninvfvMGvpzcZhjZkx0wztvGvZD9ZodEd6/sYJodPgsEQUvuMscxC/sk/zLHMStesFS4uQiFG5HKLy3Txk/bYvl67WH++Pu0Y3mQjwf9WoQzoHUkV9av4bzbVs5mGObVoMxz5pSV85kGWRnmpy3jvJ/TzZBlSwdbphmSbBnmpz0TbFnmz/ZMc33OOlumuY3dZv6CzvlFnfPLOqceDLBn5W5ryzDn7Vm5k1QcOcExJ+AVFsw4/7+vPHGiVnu4fXaplqRwcxEKNyKuZU98Ml//cZh5W45xIjndsbymvyc9moRydaOaXNWwJjX8L/PNx1K2DMMMSPbM3OCTE4IcVzbIDU55Q1FO8Mq7nT0r/9WHnP3nC1aZ2cfMAsOWJ6DZ8x/n/P3nHDdnmwLb2XKP59ivLTfkOT4peBvP8d3yfEfyBEa7PfccZWVkB8sKGAxrd4J7FpfqLhVuLkLhRsQ12ewGaw+cYt7mWH7aFkvC2cx862MiA7mqYU3a161G+7rVFHbEdRlGIUHq/E97blvHFTZ7bliz54Q4W57tyP3Me5XGMArexnL3guA6pfq1FG4uQuFGxPVl2uys3X+a3/acYMWek+yMTSrQJrqmH+3qVKNV7SCaRwbSNDyAAG8PJ1QrIkWhcHMRCjciVc/x5DRW7j3JHwdO8+ffZ9hzPKXQdnWq+9I0PICGof40CPGnYag/9UP8FHpEKgCFm4tQuBGRxLOZbDh8ho0Hz7D9WBI7Y5M4lph2wfY1/T2pVc2X2tV8zCnYh/AgH8IDvQkL8qKmnxfWqtp5WaScKNxchMKNiBTmTGoGO2OT2BWXzL4TKdlTar5OyhfibrVQ09+LkAAvavp7UtPfi5oBXlT39aSanyfV/TwI9vUk2MeDAG8PArzd8XK3lt+blkVcgMLNRSjciEhxJJ7L5MiZsxw5cy57OsvRM+eIT0ojNjGNEynplORvUQ83C4HZQSfA24NAH3cCvDzw93bH19MNH083fD3MeW9PN3w83PD2sGZ/mvNe7rnz3tnLvdytuFstCk7icorz+9u9nGoSEamUgnw8CPIJIiYyqND1WTY7J1LSOZ6UzskUczqRnM7JlAzOnM3gdGoGCWczOZ2aQdK5TFIysjAMyLQZnErN4FRq6b+d2GoBL3c3PN2t5uSW++nuZsHDzZz3cLfgbrWaP+eZ93Cz4O6W87MFdzczMLlbrdnL8yzL+dmxLreN1WrBzWLBarFgtYK71Yqb1Vznlmdbq8Xczs0tu31O2+x5t+w2Voslex6FN7kohRsRkcvg7mYlIsiHiKCijQButxukZmSRnJYzZZKUlklyWhZJaVmkpGVxLiOLsxk2zmbaOJueRVqmnXOZNs5l2kjP/kzLtJOWaTOnLDsZ2WNvAdgNHO1dldWSG3rcskNUbjjK/tmaHY7OX+ZYZwYlq8VcZ8met1pxhCmrY1lu8Cps3fnz5v5yjlWwnSV7ewvZy7L7bOXs182a26bgfnPPgyX7pXqF1ZNTg1u+4+bWb8l5H1/OOnBsaznvO1pyarXmtrXk2Z+F3HYAXh5WQgOcNyyKwo2ISDmyWi3Z/W5K9wksu90gw2YnPdNOepaN9Cw76dmhJ8NmfmbZzPlMm5FvPtNmrkvPspNlN9dl2gyy7HaybEbuMruBzWaQabdjsxtk5WxrN9vY7Ln7ttkNbIaBzW7WlmW3YzfM9xHZ7Ln7thnmPrOyl5vbXPo+n90Auy3PO1qkQmlbJ5jZD3R12vEVbkREXIDVasHbava7gcr/6Lo9T9CxG4YjGOUsz7fejmM+Z7IbuWHJbs87T75lOfvOaW8AhpG9PLutkR3ScrbL2cbIcwzDyNnOrNMx79hX7nGMAsfLc8zC2uSZt2d38DKM3FiXt72Rc55y5s87voH5vXLWG+TZFsxRMvLUYc+uMe93ztvebjccteR8ZwMDLyeP8aZwIyIiFY7VasGKBY9Cxr8UuZRKOHyuiIiIyIUp3IiIiIhLUbgRERERl6JwIyIiIi5F4UZERERcisKNiIiIuBSFGxEREXEpCjciIiLiUhRuRERExKUo3IiIiIhLUbgRERERl6JwIyIiIi5F4UZERERcisKNiIiIuBR3ZxdQ3gzDACApKcnJlYiIiEhR5fzezvk9fjFVLtwkJycDEBUV5eRKREREpLiSk5MJCgq6aBuLUZQI5ELsdjvHjh0jICAAi8VSqvtOSkoiKiqKw4cPExgYWKr7lvx0rsuPznX50bkuPzrX5ae0zrVhGCQnJxMZGYnVevFeNVXuyo3VaqV27dpleozAwED9x1JOdK7Lj851+dG5Lj861+WnNM71pa7Y5FCHYhEREXEpCjciIiLiUhRuSpGXlxfPPfccXl5ezi7F5elclx+d6/Kjc11+dK7LjzPOdZXrUCwiIiKuTVduRERExKUo3IiIiIhLUbgRERERl6JwIyIiIi5F4aaUfPDBB9SrVw9vb2+uuOIK/vjjD2eXVOlNmjSJjh07EhAQQGhoKIMHD2b37t352qSlpTF27Fhq1KiBv78/Q4cOJT4+3kkVu45///vfWCwWHn74YccynevSc/ToUW677TZq1KiBj48PLVu25M8//3SsNwyDZ599loiICHx8fOjduzd79uxxYsWVk81m45lnniE6OhofHx8aNGjAiy++mG9sIp3rkluxYgUDBgwgMjISi8XCnDlz8q0vyrk9ffo0I0aMIDAwkODgYO6++25SUlIuvzhDLts333xjeHp6Gp999pmxfft249577zWCg4ON+Ph4Z5dWqfXp08eYOnWqsW3bNmPTpk3G9ddfb9SpU8dISUlxtBk9erQRFRVlLFmyxPjzzz+NK6+80ujSpYsTq678/vjjD6NevXpGq1atjIceesixXOe6dJw+fdqoW7euMWrUKGPt2rXG/v37jYULFxp79+51tPn3v/9tBAUFGXPmzDE2b95sDBw40IiOjjbOnTvnxMorn5dfftmoUaOGMX/+fOPAgQPGzJkzDX9/f+Odd95xtNG5Lrkff/zReOqpp4xZs2YZgDF79ux864tybvv27Wu0bt3aWLNmjfHbb78ZDRs2NIYPH37ZtSnclIJOnToZY8eOdfxss9mMyMhIY9KkSU6syvUcP37cAIzly5cbhmEYCQkJhoeHhzFz5kxHm507dxqAsXr1ameVWaklJycbjRo1MhYvXmx0797dEW50rkvPP/7xD+Oqq6664Hq73W6Eh4cbr732mmNZQkKC4eXlZXz99dflUaLL6N+/v3HXXXflW3bjjTcaI0aMMAxD57o0nR9uinJud+zYYQDGunXrHG1++uknw2KxGEePHr2senRb6jJlZGSwfv16evfu7VhmtVrp3bs3q1evdmJlricxMRGA6tWrA7B+/XoyMzPznfumTZtSp04dnfsSGjt2LP379893TkHnujTNnTuXDh06cPPNNxMaGkrbtm355JNPHOsPHDhAXFxcvnMdFBTEFVdcoXNdTF26dGHJkiX89ddfAGzevJnff/+dfv36ATrXZako53b16tUEBwfToUMHR5vevXtjtVpZu3btZR2/yg2cWdpOnjyJzWYjLCws3/KwsDB27drlpKpcj91u5+GHH6Zr1660aNECgLi4ODw9PQkODs7XNiwsjLi4OCdUWbl98803bNiwgXXr1hVYp3Ndevbv38+HH37IhAkT+Ne//sW6det48MEH8fT0ZOTIkY7zWdjfKTrXxfPPf/6TpKQkmjZtipubGzabjZdffpkRI0YA6FyXoaKc27i4OEJDQ/Otd3d3p3r16pd9/hVupFIYO3Ys27Zt4/fff3d2KS7p8OHDPPTQQyxevBhvb29nl+PS7HY7HTp04JVXXgGgbdu2bNu2jSlTpjBy5EgnV+daZsyYwVdffcX06dOJiYlh06ZNPPzww0RGRupcuzjdlrpMNWvWxM3NrcBTI/Hx8YSHhzupKtcybtw45s+fz6+//krt2rUdy8PDw8nIyCAhISFfe5374lu/fj3Hjx+nXbt2uLu74+7uzvLly3n33Xdxd3cnLCxM57qURERE0Lx583zLmjVrxqFDhwAc51N/p1y+xx9/nH/+85/ceuuttGzZkttvv51HHnmESZMmATrXZako5zY8PJzjx4/nW5+VlcXp06cv+/wr3FwmT09P2rdvz5IlSxzL7HY7S5YsoXPnzk6srPIzDINx48Yxe/Zsli5dSnR0dL717du3x8PDI9+53717N4cOHdK5L6ZevXqxdetWNm3a5Jg6dOjAiBEjHPM616Wja9euBV5p8Ndff1G3bl0AoqOjCQ8Pz3euk5KSWLt2rc51MZ09exarNf+vOTc3N+x2O6BzXZaKcm47d+5MQkIC69evd7RZunQpdrudK6644vIKuKzuyGIYhvkouJeXlzFt2jRjx44dxn333WcEBwcbcXFxzi6tUhszZowRFBRkLFu2zIiNjXVMZ8+edbQZPXq0UadOHWPp0qXGn3/+aXTu3Nno3LmzE6t2HXmfljIMnevS8scffxju7u7Gyy+/bOzZs8f46quvDF9fX+PLL790tPn3v/9tBAcHGz/88IOxZcsWY9CgQXo8uQRGjhxp1KpVy/Eo+KxZs4yaNWsaTzzxhKONznXJJScnGxs3bjQ2btxoAMabb75pbNy40Th48KBhGEU7t3379jXatm1rrF271vj999+NRo0a6VHwiuS9994z6tSpY3h6ehqdOnUy1qxZ4+ySKj2g0Gnq1KmONufOnTMeeOABo1q1aoavr68xZMgQIzY21nlFu5Dzw43OdemZN2+e0aJFC8PLy8to2rSp8fHHH+dbb7fbjWeeecYICwszvLy8jF69ehm7d+92UrWVV1JSkvHQQw8ZderUMby9vY369esbTz31lJGenu5oo3Ndcr/++muhf0ePHDnSMIyindtTp04Zw4cPN/z9/Y3AwEDjzjvvNJKTky+7Noth5HlVo4iIiEglpz43IiIi4lIUbkRERMSlKNyIiIiIS1G4EREREZeicCMiIiIuReFGREREXIrCjYiIiLgUhRsRcaqHHnqI++67z/FKfBGRy6VwIyJOc/jwYZo0acJHH31UYAwgEZGS0huKRURExKXon0oiUu5GjRqFxWIpMPXt29fZpYmIC3B3dgEiUjX17duXqVOn5lvm5eXlpGpExJXoyo2IOIWXlxfh4eH5pmrVqgFgsVj48MMP6devHz4+PtSvX5/vvvsu3/Zbt27lmmuuwcfHhxo1anDfffeRkpKSr81nn31GTEwMXl5eREREMG7cOMe6N998k5YtW+Ln50dUVBQPPPBAvu0PHjzIgAEDqFatGn5+fsTExPDjjz+W4RkRkdKicCMiFdIzzzzD0KFD2bx5MyNGjODWW29l586dAKSmptKnTx+qVavGunXrmDlzJr/88ku+8PLhhx8yduxY7rvvPrZu3crcuXNp2LChY73VauXdd99l+/bt/O9//2Pp0qU88cQTjvVjx44lPT2dFStWsHXrVl599VX8/f3L7wSISMkZIiLlbOTIkYabm5vh5+eXb3r55ZcNwzAMwBg9enS+ba644gpjzJgxhmEYxscff2xUq1bNSElJcaxfsGCBYbVajbi4OMMwDCMyMtJ46qmnilzTzJkzjRo1ajh+btmypTFx4sQSf0cRcR71uRERp+jZsycffvhhvmXVq1d3zHfu3Dnfus6dO7Np0yYAdu7cSevWrfHz83Os79q1K3a7nd27d2OxWDh27Bi9evW64PF/+eUXJk2axK5du0hKSiIrK4u0tDTOnj2Lr68vDz74IGPGjGHRokX07t2boUOH0qpVq1L45iJS1nRbSkScws/Pj4YNG+ab8oaby+Hj43PR9X///Tc33HADrVq14vvvv2f9+vV88MEHAGRkZABwzz33sH//fm6//Xa2bt1Khw4deO+990qlPhEpWwo3IlIhrVmzpsDPzZo1A6BZs2Zs3ryZ1NRUx/qVK1ditVpp0qQJAQEB1KtXjyVLlhS67/Xr12O323njjTe48sorady4MceOHSvQLioqitGjRzNr1iweffRRPvnkk1L8hiJSVnRbSkScIj09nbi4uHzL3N3dqVmzJgAzZ86kQ4cOXHXVVXz11Vf88ccffPrppwCMGDGC5557jpEjRzJx4kROnDjB+PHjuf322wkLCwNg4sSJjB49mtDQUPr160dycjIrV65k/PjxNGzYkMzMTN577z0GDBjAypUrmTJlSr5aHn74Yfr160fjxo05c+YMv/76qyNciUgF5+xOPyJS9YwcOdIACkxNmjQxDMPsUPzBBx8Y1157reHl5WXUq1fP+Pbbb/PtY8uWLUbPnj0Nb29vo3r16sa9995rJCcn52szZcoUo0mTJoaHh4cRERFhjB8/3rHuzTffNCIiIgwfHx+jT58+xueff24AxpkzZwzDMIxx48YZDRo0MLy8vIyQkBDj9ttvN06ePFm2J0ZESoWGXxCRCsdisTB79mwGDx7s7FJEpBJSnxsRERFxKQo3IiIi4lLUoVhEKhzdLReRy6ErNyIiIuJSFG5ERETEpSjciIiIiEtRuBERERGXonAjIiIiLkXhRkRERFyKwo2IiIi4FIUbERERcSkKNyIiIuJS/h8rkn98k7KV1wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar el loss durante el entrenamiento\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRUEBA 1: Tmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2604413485056.0000 - val_loss: 2721400487936.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2588717875200.0000 - val_loss: 2721386070016.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2549174501376.0000 - val_loss: 2721317912576.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2519708467200.0000 - val_loss: 2721093779456.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2596577476608.0000 - val_loss: 2720541966336.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2520420974592.0000 - val_loss: 2719421300736.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2540074434560.0000 - val_loss: 2717412229120.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2587521974272.0000 - val_loss: 2714191003648.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528323043328.0000 - val_loss: 2709304115200.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2608842670080.0000 - val_loss: 2702294384640.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2507721408512.0000 - val_loss: 2692684972032.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2475571281920.0000 - val_loss: 2679929044992.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2538263019520.0000 - val_loss: 2663529054208.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2500504846336.0000 - val_loss: 2642819940352.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2521176997888.0000 - val_loss: 2617454624768.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2453310013440.0000 - val_loss: 2586817069056.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2382005010432.0000 - val_loss: 2550317187072.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2315011751936.0000 - val_loss: 2507972280320.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2300997795840.0000 - val_loss: 2458876903424.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2232706924544.0000 - val_loss: 2403228975104.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2229398929408.0000 - val_loss: 2340572102656.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2122998743040.0000 - val_loss: 2271079038976.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2126291795968.0000 - val_loss: 2194922143744.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2014937219072.0000 - val_loss: 2112283738112.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1917113597952.0000 - val_loss: 2022570721280.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1858156756992.0000 - val_loss: 1928101232640.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1691694006272.0000 - val_loss: 1828398694400.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1626953220096.0000 - val_loss: 1723799437312.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1523607404544.0000 - val_loss: 1616972873728.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1398539288576.0000 - val_loss: 1507864084480.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1334019883008.0000 - val_loss: 1397800566784.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1225879846912.0000 - val_loss: 1288775270400.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1088790069248.0000 - val_loss: 1182605901824.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 986042073088.0000 - val_loss: 1079619289088.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 878356791296.0000 - val_loss: 979798065152.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 832954302464.0000 - val_loss: 886604300288.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 755802374144.0000 - val_loss: 800860995584.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 684223234048.0000 - val_loss: 723681083392.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 622701510656.0000 - val_loss: 655061680128.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 495814082560.0000 - val_loss: 593556078592.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 451071246336.0000 - val_loss: 540788490240.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 395381800960.0000 - val_loss: 496075538432.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 361064660992.0000 - val_loss: 459745492992.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 307949830144.0000 - val_loss: 429825458176.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 323718676480.0000 - val_loss: 405514944512.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 285639376896.0000 - val_loss: 385987411968.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 278864068608.0000 - val_loss: 371235618816.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 229060526080.0000 - val_loss: 360420999168.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 262469992448.0000 - val_loss: 351577309184.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 232645607424.0000 - val_loss: 345716719616.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 239004614656.0000 - val_loss: 341020278784.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 258268545024.0000 - val_loss: 337105813504.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 251579531264.0000 - val_loss: 334802124800.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 218064519168.0000 - val_loss: 332963577856.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 236606357504.0000 - val_loss: 331365318656.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 239126396928.0000 - val_loss: 330100637696.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 227260235776.0000 - val_loss: 329170223104.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 240171876352.0000 - val_loss: 328434712576.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 218167459840.0000 - val_loss: 328111456256.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 241223368704.0000 - val_loss: 327612694528.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 227905765376.0000 - val_loss: 327461076992.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 238114619392.0000 - val_loss: 326854180864.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 229210816512.0000 - val_loss: 326655016960.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259192291328.0000 - val_loss: 326475153408.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 240657104896.0000 - val_loss: 326326648832.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 238049050624.0000 - val_loss: 326321143808.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 236267257856.0000 - val_loss: 326200885248.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 218109984768.0000 - val_loss: 326042845184.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 228776460288.0000 - val_loss: 326053691392.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 246063693824.0000 - val_loss: 326097502208.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 206464614400.0000 - val_loss: 325958795264.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 248640454656.0000 - val_loss: 326052675584.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 245177188352.0000 - val_loss: 325997395968.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 250596507648.0000 - val_loss: 325842403328.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 229566201856.0000 - val_loss: 325645631488.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 231971307520.0000 - val_loss: 325508005888.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 233159950336.0000 - val_loss: 325763072000.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 234577870848.0000 - val_loss: 325597364224.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 225628110848.0000 - val_loss: 325661196288.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 218419658752.0000 - val_loss: 325809373184.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 247667474432.0000 - val_loss: 325722210304.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 237416464384.0000 - val_loss: 325917474816.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 233293545472.0000 - val_loss: 325967609856.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 258551840768.0000 - val_loss: 325436243968.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 247768743936.0000 - val_loss: 325333090304.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 260746182656.0000 - val_loss: 325480349696.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 223993839616.0000 - val_loss: 325577080832.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 242239881216.0000 - val_loss: 325539954688.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 228767842304.0000 - val_loss: 325836406784.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 236681396224.0000 - val_loss: 325588582400.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 216809078784.0000 - val_loss: 325373362176.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 247393632256.0000 - val_loss: 325350359040.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 265794977792.0000 - val_loss: 325381685248.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 221703913472.0000 - val_loss: 325533728768.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 214015803392.0000 - val_loss: 325396398080.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 242099027968.0000 - val_loss: 325525569536.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 224422772736.0000 - val_loss: 325495586816.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 247032266752.0000 - val_loss: 325292982272.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 247649927168.0000 - val_loss: 324936597504.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 215422255104.0000 - val_loss: 325574262784.0000\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 231343357952.0000\n",
      "Test Loss: 244693352448.0\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MAE: 383231.9657960199\n",
      "RMSE: 494664.9159538417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = df[['tmed']]  # Variables independientes (continuas y dummy)\n",
    "y = df['consumo']  # Variable dependiente (demanda de electricidad)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Definir el modelo secuencial\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada (input) y capas ocultas\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Capa de salida (una neurona para predicción continua)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "test_loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRUEBA 2: 'tmed', 'prec', 'velmedia', 'poblacion', 'PIB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2630829473792.0000 - val_loss: 2721393147904.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2489622200320.0000 - val_loss: 2721345961984.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2568909225984.0000 - val_loss: 2721167966208.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2495640502272.0000 - val_loss: 2720646299648.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2528976568320.0000 - val_loss: 2719421562880.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2636871106560.0000 - val_loss: 2717016129536.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2564043309056.0000 - val_loss: 2712804786176.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2605433749504.0000 - val_loss: 2706013159424.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2591121211392.0000 - val_loss: 2695811301376.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2523338113024.0000 - val_loss: 2681262047232.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2437141757952.0000 - val_loss: 2661312626688.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2434263416832.0000 - val_loss: 2635105304576.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2479836889088.0000 - val_loss: 2601729654784.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2409243607040.0000 - val_loss: 2560155451392.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2351522119680.0000 - val_loss: 2509354303488.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2348772229120.0000 - val_loss: 2449305239552.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2275147776000.0000 - val_loss: 2378644848640.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2138501152768.0000 - val_loss: 2297678790656.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2061270646784.0000 - val_loss: 2205881729024.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1995028430848.0000 - val_loss: 2103273848832.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1945242697728.0000 - val_loss: 1990834257920.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1749849997312.0000 - val_loss: 1871286632448.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1601404141568.0000 - val_loss: 1744428335104.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1566493376512.0000 - val_loss: 1610812096512.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1390991769600.0000 - val_loss: 1476248600576.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1258304438272.0000 - val_loss: 1340765241344.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1140319584256.0000 - val_loss: 1208265474048.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 963204087808.0000 - val_loss: 1082570178560.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 852272807936.0000 - val_loss: 962282979328.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 754346426368.0000 - val_loss: 852585480192.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 700516925440.0000 - val_loss: 754599723008.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 567544446976.0000 - val_loss: 671015239680.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 497676746752.0000 - val_loss: 600515411968.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 432026353664.0000 - val_loss: 540426895360.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 384417955840.0000 - val_loss: 492067717120.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 329191292928.0000 - val_loss: 453110857728.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 317149380608.0000 - val_loss: 422826999808.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 322497052672.0000 - val_loss: 399356100608.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 252051849216.0000 - val_loss: 381047406592.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 257820213248.0000 - val_loss: 366213758976.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 228332453888.0000 - val_loss: 355307225088.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 249428164608.0000 - val_loss: 346119798784.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 251096825856.0000 - val_loss: 338227101696.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 251253211136.0000 - val_loss: 332117901312.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 242229182464.0000 - val_loss: 326329270272.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 247507386368.0000 - val_loss: 321853390848.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 228348559360.0000 - val_loss: 318100733952.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 247746379776.0000 - val_loss: 314533216256.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 200381464576.0000 - val_loss: 310932865024.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 208487890944.0000 - val_loss: 307851132928.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 224590118912.0000 - val_loss: 305286086656.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 224410419200.0000 - val_loss: 302862991360.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 217971408896.0000 - val_loss: 299948933120.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 219413086208.0000 - val_loss: 297515253760.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 227300737024.0000 - val_loss: 295314259968.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 214959538176.0000 - val_loss: 293837373440.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 203880906752.0000 - val_loss: 291435184128.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 222347870208.0000 - val_loss: 289781809152.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 201538142208.0000 - val_loss: 287878643712.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 208575135744.0000 - val_loss: 286185947136.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 202744119296.0000 - val_loss: 284884992000.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 184218746880.0000 - val_loss: 282942701568.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 206010482688.0000 - val_loss: 281649446912.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 212314488832.0000 - val_loss: 280312512512.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 208764076032.0000 - val_loss: 278415900672.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 197138415616.0000 - val_loss: 277037449216.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 186208190464.0000 - val_loss: 275814776832.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 219574009856.0000 - val_loss: 274088755200.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 205114114048.0000 - val_loss: 273241276416.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 180141948928.0000 - val_loss: 272620224512.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 201714679808.0000 - val_loss: 270853570560.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 207095087104.0000 - val_loss: 269812170752.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 197175148544.0000 - val_loss: 268903399424.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 201878929408.0000 - val_loss: 267538972672.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 198574342144.0000 - val_loss: 266424172544.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 188931981312.0000 - val_loss: 265020489728.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 185069092864.0000 - val_loss: 264503771136.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 189454073856.0000 - val_loss: 264094482432.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 185676120064.0000 - val_loss: 263102775296.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 196195581952.0000 - val_loss: 261656363008.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 198898876416.0000 - val_loss: 260708319232.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 174759329792.0000 - val_loss: 260290478080.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 196412801024.0000 - val_loss: 259427106816.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 188584820736.0000 - val_loss: 258424897536.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 193984675840.0000 - val_loss: 257986740224.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 156840181760.0000 - val_loss: 257406107648.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 189402906624.0000 - val_loss: 256529596416.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 194343469056.0000 - val_loss: 255599706112.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 191647973376.0000 - val_loss: 255137579008.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 184789745664.0000 - val_loss: 254611128320.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 172698910720.0000 - val_loss: 253526786048.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 179109085184.0000 - val_loss: 253054550016.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 185973948416.0000 - val_loss: 252899540992.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 169722216448.0000 - val_loss: 252555739136.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 187526119424.0000 - val_loss: 251223261184.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 169984884736.0000 - val_loss: 250284376064.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 174117486592.0000 - val_loss: 249669468160.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 183665983488.0000 - val_loss: 249380339712.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 195460743168.0000 - val_loss: 248454184960.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 179707232256.0000 - val_loss: 247906140160.0000\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 173315014656.0000\n",
      "Test Loss: 169892315136.0\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MAE: 311401.7661691542\n",
      "RMSE: 412179.9589003943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df[['tmed', 'prec', 'velmedia', 'poblacion', 'PIB']]  # Variables independientes (continuas y dummy)\n",
    "y = df['consumo']  # Variable dependiente (demanda de electricidad)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Definir el modelo secuencial\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada (input) y capas ocultas\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Capa de salida (una neurona para predicción continua)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "test_loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRUEBA 3: Tmed y fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna de fecha a formato datetime\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Asegúrate de que la columna de fecha está en formato datetime\n",
    "df['fecha'] = pd.to_datetime(df['fecha'], format='%Y')  # Ajusta el formato según tus datos\n",
    "\n",
    "# Convertir la columna de fecha a timestamp\n",
    "df['fecha'] = df['fecha'].astype(np.int64) // 10**9  # Convertir a dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2593722990592.0000 - val_loss: 2721398128640.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2536688582656.0000 - val_loss: 2721373487104.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2587694465024.0000 - val_loss: 2721266270208.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2556746530816.0000 - val_loss: 2720939376640.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2558993891328.0000 - val_loss: 2720166576128.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2531344777216.0000 - val_loss: 2718623334400.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2574563934208.0000 - val_loss: 2715896250368.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2498053537792.0000 - val_loss: 2711507697664.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2507996135424.0000 - val_loss: 2704878075904.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2541932511232.0000 - val_loss: 2695353860096.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2592460767232.0000 - val_loss: 2682305642496.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2527011799040.0000 - val_loss: 2665025372160.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2453460221952.0000 - val_loss: 2642636177408.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2395754463232.0000 - val_loss: 2614690840576.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2419958743040.0000 - val_loss: 2580079968256.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2411906727936.0000 - val_loss: 2538552164352.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2375084670976.0000 - val_loss: 2489109708800.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2383739879424.0000 - val_loss: 2431691259904.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2263871389696.0000 - val_loss: 2365828628480.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2315859001344.0000 - val_loss: 2290909446144.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2097288577024.0000 - val_loss: 2208747749376.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2094172864512.0000 - val_loss: 2117542084608.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1957163040768.0000 - val_loss: 2018687713280.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1876510638080.0000 - val_loss: 1913052987392.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1739355062272.0000 - val_loss: 1800361213952.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1623096950784.0000 - val_loss: 1682886623232.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1480420884480.0000 - val_loss: 1562805927936.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1405927030784.0000 - val_loss: 1440056213504.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1266826739712.0000 - val_loss: 1317488033792.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115185348608.0000 - val_loss: 1196509626368.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1086382342144.0000 - val_loss: 1078854942720.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 961331462144.0000 - val_loss: 967037091840.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 826008666112.0000 - val_loss: 863190646784.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 721594613760.0000 - val_loss: 767700893696.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 632926371840.0000 - val_loss: 683249172480.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 558690009088.0000 - val_loss: 608864108544.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 462989459456.0000 - val_loss: 544599113728.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 426920509440.0000 - val_loss: 492055265280.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 387955982336.0000 - val_loss: 448978419712.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 354544517120.0000 - val_loss: 414549737472.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 319377276928.0000 - val_loss: 389623054336.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 289478148096.0000 - val_loss: 370596511744.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 267713970176.0000 - val_loss: 356548444160.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 268188450816.0000 - val_loss: 346690060288.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 219805941760.0000 - val_loss: 339148570624.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 248051630080.0000 - val_loss: 334436335616.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 252272066560.0000 - val_loss: 331007623168.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 229447892992.0000 - val_loss: 328581152768.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 238405058560.0000 - val_loss: 327195426816.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 231248347136.0000 - val_loss: 326319734784.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 219892449280.0000 - val_loss: 325915738112.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 262943129600.0000 - val_loss: 325510856704.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 225870446592.0000 - val_loss: 325267849216.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 235115233280.0000 - val_loss: 325213552640.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 230899597312.0000 - val_loss: 325260869632.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 239111061504.0000 - val_loss: 325080547328.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 229615910912.0000 - val_loss: 324966645760.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 236878200832.0000 - val_loss: 324899340288.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 242455576576.0000 - val_loss: 324940333056.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 244549107712.0000 - val_loss: 325162172416.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 251241693184.0000 - val_loss: 325142380544.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 227698458624.0000 - val_loss: 325196578816.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 247098998784.0000 - val_loss: 325204574208.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 234635624448.0000 - val_loss: 325131468800.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 240723181568.0000 - val_loss: 325049810944.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 249598738432.0000 - val_loss: 325014388736.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 238892711936.0000 - val_loss: 324872601600.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 245261942784.0000 - val_loss: 325054758912.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 233793388544.0000 - val_loss: 324989288448.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 223597428736.0000 - val_loss: 325064916992.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 234570432512.0000 - val_loss: 324887871488.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 235936415744.0000 - val_loss: 325164335104.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 222943346688.0000 - val_loss: 325197365248.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 237177667584.0000 - val_loss: 325197791232.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 245553610752.0000 - val_loss: 325360877568.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 245636202496.0000 - val_loss: 325194416128.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 232770043904.0000 - val_loss: 325324472320.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 240721821696.0000 - val_loss: 325254414336.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 214896541696.0000 - val_loss: 325305368576.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 218413776896.0000 - val_loss: 325375066112.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 254193106944.0000 - val_loss: 325121703936.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 239273623552.0000 - val_loss: 325039620096.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 221653401600.0000 - val_loss: 325433360384.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 204403965952.0000 - val_loss: 325656608768.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 238929494016.0000 - val_loss: 325432442880.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 231301611520.0000 - val_loss: 325368053760.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 226620358656.0000 - val_loss: 325551554560.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 248261148672.0000 - val_loss: 325264474112.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 222654791680.0000 - val_loss: 325246386176.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 228152852480.0000 - val_loss: 325328633856.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 251625046016.0000 - val_loss: 325282988032.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 232351531008.0000 - val_loss: 325068423168.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 238127824896.0000 - val_loss: 325161910272.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 248481087488.0000 - val_loss: 325093425152.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 238455504896.0000 - val_loss: 324745461760.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 254716657664.0000 - val_loss: 325098176512.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 245295874048.0000 - val_loss: 325067472896.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 231852441600.0000 - val_loss: 325067440128.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 239273050112.0000 - val_loss: 325271584768.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 238012104704.0000 - val_loss: 325054201856.0000\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 233638707200.0000\n",
      "Test Loss: 246227222528.0\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MAE: 385417.4940920398\n",
      "RMSE: 496212.8609768774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = df[['tmed']]  # Variables independientes (continuas y dummy)\n",
    "y = df['consumo']  # Variable dependiente (demanda de electricidad)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Definir el modelo secuencial\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada (input) y capas ocultas\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Capa de salida (una neurona para predicción continua)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "test_loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRUEBA 4: INDUSTRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Asegúrate de que la columna de fecha está en formato datetime\n",
    "df['fecha'] = pd.to_datetime(df['fecha'], format='%Y')  # Ajusta el formato según tus datos\n",
    "\n",
    "# Convertir la columna de fecha a timestamp\n",
    "df['fecha'] = df['fecha'].astype(np.int64) // 10**9  # Convertir a dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tmed']]  # Variables independientes (continuas y dummy)\n",
    "y = df['consumo']  # Variable dependiente (demanda de electricidad)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Definir el modelo secuencial\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada (input) y capas ocultas\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Capa de salida (una neurona para predicción continua)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "test_loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RMSE: {rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
